<!DOCTYPE html>
<!--[if IE 9]><html class="ie9" lang="en"><![endif]-->
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <meta name="citation_pii" content="S0003687017302867">
<meta name="citation_issn" content="0003-6870">
<meta name="citation_volume" content="69">
<meta name="citation_lastpage" content="16">
<meta name="citation_publisher" content="Elsevier">
<meta name="citation_firstpage" content="10">
<meta name="citation_journal_title" content="Applied Ergonomics">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.apergo.2017.12.020">
<meta name="dc.identifier" content="10.1016/j.apergo.2017.12.020">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="The promising technology of stereoscopic displays is interesting to explore because 3D virtual applications are widely known. Thus, this study investiâ€¦">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-fx1.jpg">
<meta name="citation_title" content="The effect of parallax on eye fixation parameter in projection-based stereoscopic displays">
<meta property="og:title" content="The effect of parallax on eye fixation parameter in projection-based stereoscopic displays">
<meta name="citation_publication_date" content="2018/05/01">
<meta name="citation_online_date" content="2018/01/06">
<meta name="citation_pdf_url" content="https://www.sciencedirect.com/science/article/pii/S0003687017302867/pdfft?md5=af42c555a80c115353af2b16ffca109c&amp;pid=1-s2.0-S0003687017302867-main.pdf">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR">
      <title>The effect of parallax on eye fixation parameter in projection-based stereoscopic displays - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0003687017302867">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="arp.css">
      <link rel="dns-prefetch" href="https://w.usabilla.com/">
      <link rel="dns-prefetch" href="https://www.deepdyve.com/">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com/">
      <iframe src="javascript:false" title="" style="width: 0px; height: 0px; border: 0px none; display: none;"></iframe><script>
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Applied Ergonomics"};
        window.pageData = {"content":[{"entitlementType":"package","format":"MIME-XHTML","id":"sd:article:pii:S0003687017302867","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0003-6870","volumeNumber":"69","suppl":"C"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1544570426690,"loadTime":""},"visitor":{"accessType":"ae:ANON_SHIBBOLETH","accountId":"ae:47346","accountName":"ae:Galway Mayo Institute of Technology","loginStatus":"anonymous","userId":"ae:12543123","ipAddress":"89.127.20.127","appSessionId":"6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb"}};
        window.arp = {
          config: {"reduxLogging":false,"assetsBaseUrl":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/f25aea1948592b0501c1d74a88db29cee53e602a","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","googleMapsApiKey":"AIzaSyAV0ZB1XWtRlzJ06pJAW74Oan87lQVgBHY","bosUrl":"https://feedback.recs.d.elsevier.com/raw/events","bosTimeOut":60000,"strictMode":false,"facebookAppId":"269707397153187","enableGoogleScholarLinks":true,"enableEzProxyForEnhancedReader":false},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
      </script>
      
      <!-- begin mPulse embed code -->
      <script>
        (function(){
          if(window.BOOMR && window.BOOMR.version){return;}
          var dom,doc,where,iframe = document.createElement('iframe'),win = window;

          function boomerangSaveLoadTime(e) {
            win.BOOMR_onload=(e && e.timeStamp) || new Date().getTime();
          }
          if (win.addEventListener) {
            win.addEventListener("load", boomerangSaveLoadTime, false);
          } else if (win.attachEvent) {
            win.attachEvent("onload", boomerangSaveLoadTime);
          }

          iframe.src = "javascript:false";
          iframe.title = ""; iframe.role="presentation";
          (iframe.frameElement || iframe).style.cssText = "width:0;height:0;border:0;display:none;";
          where = document.getElementsByTagName('script')[0];
          where.parentNode.insertBefore(iframe, where);

          try {
            doc = iframe.contentWindow.document;
          } catch(e) {
            dom = document.domain;
            iframe.src="javascript:var d=document.open();d.domain='"+dom+"';void(0);";
            doc = iframe.contentWindow.document;
          }
          doc.open()._l = function() {
            var js = this.createElement("script");
            if(dom) this.domain = dom;
            js.id = "boomr-if-as";
            js.src = 'https://c.go-mpulse.net/boomerang/2FBN2-NKMGU-EJKY8-ZANKZ-SUJZF';
            BOOMR_lstart=new Date().getTime();
            this.body.appendChild(js);
          };
          doc.write('<body onload="document._l();">');
          doc.close();
        })();
      </script>
      <!-- end mPulse embed code -->
    <script src="satellite-5b3b560664746d57b70018c6.js"></script><script src="s-code-contents-9c0358adbc3b5986e210099b3bf1d427fc5bd286.js"></script><link rel="preload" href="integrator.js"><script type="text/javascript" src="integrator.js"></script><link rel="preload" href="integrator_002.js"><script type="text/javascript" src="integrator_002.js"></script><script src="pubads_impl_284.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style></head>
    <body><div style="display: none;" id="lightningjs-usabilla_live"><div><iframe id="lightningjs-frame-usabilla_live" frameborder="0"></iframe></div></div>
      <a class="sr-only sr-only-focusable" href="#app">Skip to main content</a>
      <!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service.elsevier.com/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
      <div data-iso-key="_0"><div class="App" id="app" data-reactroot=""><div class="page"><section><div id="header"><div class="customer-banner u-hide-from-lg"><div id="library-banner" class="desktop-library-banner u-show-from-lg u-margin-xs-right"><div class="customer-banner-container text-xs"><div class="text-customer-banner text-xs u-bg-white u-fill-black u-clr-black" style="border-color:#c9c9c9;border-style:solid;border-width:1px;text-align:center;height:60px;width:234px;overflow:hidden"><span class="u-display-block">Brought to you by:</span><span id="library-banner-text" class="branded"><a target="_blank" href="http://gmitlib.gmit.ie/" rel="nofollow">GMIT Library</a></span></div></div></div><div class="mobile-library-banner move-top move-center u-bg-grey1 u-padding-xxl-ver u-hide-from-lg u-display-block" id="mobile-library-banner" style="width:100%;position:relative"><div class="customer-banner-container text-xs move-middle move-center"><div class="text-customer-banner text-xs u-bg-white u-fill-black u-clr-black" style="border-color:#c9c9c9;border-style:solid;border-width:1px;text-align:center;height:60px;width:234px;overflow:hidden"><span class="u-display-block">Brought to you by:</span><span id="library-banner-text" class="branded"><a target="_blank" href="http://gmitlib.gmit.ie/" rel="nofollow">GMIT Library</a></span></div></div><button id="close-library-banner" class="button button-anchor move-top move-right u-padding-s-right u-clr-grey7" type="button"><span class="button-text"><span class="u-hide-visually">Close</span></span><svg focusable="false" viewBox="0 0 70 128" width="13.125" height="24" class="icon icon-cross"><path d="m68.94 36.12l-6.94-7.12-27 27-27-27-7 7 27 27-27 27 7 7 27-27 27 27 7-7-27-27 26.94-26.88"></path></svg></button></div></div><div class="u-bg-white" style="overflow:visible" role="banner"><div class="els-header" style="min-height:80px"><a id="els-main-title-link" href="https://www.sciencedirect.com/"><svg id="els-header-wordmark" class="u-fill-orange u-margin-l-top u-margin-s-left u-margin-l-left-from-sm" viewBox="-3334 3439.4 163 26" style="width:163px;height:24px"><title>ScienceDirect</title><g transform="scale(.8125,.8125)"><path d="M-4099.8,4240.4c0-1.8,1-3.6,4.4-3.6c1.7,0,3.7,0.5,5.5,1.6l0.2-3.1c-1.7-0.7-3.5-1.1-5.6-1.1 c-5.5,0-7.8,2.9-7.8,6.4c0,6.6,10.4,7.2,10.4,11.7c0,1.8-1.4,3.6-4.6,3.6c-2,0-4-0.7-5.6-1.6l-0.4,3.1c1.8,0.9,4.2,1.2,6.1,1.2 c5,0,7.9-2.9,7.9-6.5C-4089.4,4245.8-4099.8,4244.9-4099.8,4240.4"></path><path id="c" d="M-4080.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4083.1,4244.3-4080.6,4243.1-4080.3,4242.9"></path><path id="i" d="M-4068,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-4066,4234.1-4067,4233.1-4068,4233.1 M-4069.5,4258.1h3v-17.7h-3V4258.1z"></path><path id="e" d="M-4057.1,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4058.7,4244.4-4057.4,4243.3-4057.1,4243.1z M-4047.5,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4 c0,5.8,3.5,9.2,7.9,9.2c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-4047.5,4248.9"></path><path d="M-4034.8,4240c-2.6,0-4.3,1.3-5.7,3.1l-0.5-2.6h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.5v12.8h3v-11.9 c0.8-1.1,2.5-2.9,2.9-3.1c0.3-0.2,1.5-0.5,2.5-0.5c2.7,0,2.9,1.4,3,4c0,1.4,0,3.7,0,3.7c0,3.5-0.1,7.6-0.1,7.6h3 c0,0,0.1-5.3,0.1-8.2c0-1.8,0-3.5-0.1-5.3C-4029.5,4241-4031.6,4240-4034.8,4240"></path><path d="M-3982.5,4255.6h-4.4v-18.6h4.8c6.4,0,8.2,5.2,8.2,9.2C-3973.8,4252.2-3976.5,4255.6-3982.5,4255.6z M-3981.6,4234.6h-8.4v23.5h8.1c8.6,0,11.5-6.7,11.5-11.9C-3970.4,4240.9-3973.2,4234.6-3981.6,4234.6"></path><path d="M-3950.5,4240c-1.9,0-3.4,1.7-4,3.2l-0.5-2.8h-2.8l0.2,1.4c0.1,0.9,0.2,2.1,0.2,3.4v12.8h3v-11.2 c0.6-1.5,2-4.4,3.7-4.4c1.1,0,1.2,1.2,1.2,1.4l2.5-0.7v-0.2c0,0,0-0.2-0.1-0.5C-3947.4,4240.9-3948.5,4240-3950.5,4240"></path><path d="M-3903.5,4255.2c-1.1,0.4-2,0.8-3,0.8c-1.4,0-1.9-0.8-1.9-2.8v-10.4h4.6v-2.3h-4.6v-4.7h-2.9v4.7h-3.2v2.3h3.2 v11.4c0,3.1,1.6,4.4,3.9,4.4c1.4,0,3-0.5,4.1-0.9L-3903.5,4255.2"></path><g><path d="M-3923.3,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-3926.1,4244.3-3923.6,4243.1-3923.3,4242.9"></path></g><g><path d="M-3941.6,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-3943.2,4244.4-3941.8,4243.3-3941.6,4243.1z M-3932,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3932,4248.9"></path></g><g><path d="M-3964.5,4233.1c-1.1,0-1.9,1.1-1.9,2.1c0,1.1,0.9,2.2,1.9,2.2s2-1.1,2-2.2 C-3962.5,4234.1-3963.4,4233.1-3964.5,4233.1 M-3965.9,4258.1h3v-17.7h-3V4258.1z"></path></g><g><path d="M-4004.4,4243.1c0.2-0.2,1.5-0.5,2.3-0.5c2.9,0,4.4,0.6,4.6,4.1h-8.8 C-4006,4244.4-4004.6,4243.3-4004.4,4243.1z M-3994.7,4248.9c0-6.1-1.7-8.9-7.1-8.9c-4.6,0-7.9,3.3-7.9,9.4c0,5.8,3.5,9.2,7.9,9.2 c3.3,0,5.1-0.7,6.7-1.7l-0.2-2.7c-1.1,0.9-3.5,1.8-5.5,1.8c-3.7,0-5.8-2.3-5.8-6.5v-0.6L-3994.7,4248.9"></path></g><g><path d="M-4019.1,4242.9c0.3-0.1,0.8-0.3,2-0.3c2,0,2.9,0.4,2.9,1.9h2.8c0-0.4,0-0.9,0-1.3 c-0.3-2.3-2.5-3.2-5.8-3.2c-3.6,0-7.9,2.7-7.9,9.2c0,6.2,3.3,9.4,7.8,9.4c2,0,4.1-0.4,5.9-1.6l-0.2-2.7c-1.2,1-3.4,1.8-4.8,1.8 c-2.5,0-5.4-2-5.4-7C-4021.8,4244.3-4019.4,4243.1-4019.1,4242.9"></path></g></g></svg></a><div class="move-right u-display-inline-block"><nav class="u-clr-grey8 u-show-from-md"><div class="u-display-inline-block" style="margin-top:24px"><span><span class="u-margin-l-right"><a class="anchor qa-journals-and-books u-margin-l-right anchor-has-inherit-color" href="https://www.sciencedirect.com/browse/journals-and-books" id="els-header-navigation-section-journals-and-books" style="border-bottom:"><span class="anchor-text">Journals &amp; Books</span></a></span></span><div class="popover qa-signin-dropdown" id="user-popover"><div id="popover-trigger-user-popover"><button class="button button-anchor u-padding-0 u-clr-grey7" title="My account" aria-label="Open details and settings dropdown" role="button" aria-haspopup="true" aria-expanded="false" type="button"><span class="button-text"><span class="text-s" style="display:table-cell;max-width:175px;text-overflow:ellipsis;overflow:hidden;white-space:nowrap">My account</span></span><svg focusable="false" viewBox="0 0 106 128" style="margin-top:-12px" width="19.875" height="24" class="icon icon-person u-clr-grey8"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg></button></div></div><span><a class="anchor u-margin-l-hor help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" id="help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text"><svg focusable="false" viewBox="0 0 114 128" style="margin-top:-10px" width="21.375" height="24" class="icon icon-help"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg></span></a></span></div><div class="customer-banner u-show-from-lg move-top move-right u-display-inline-block u-position-relative"><div id="library-banner" class="desktop-library-banner u-show-from-lg u-margin-xs-right"><div class="customer-banner-container text-xs"><div class="text-customer-banner text-xs u-bg-white u-fill-black u-clr-black" style="border-color:#c9c9c9;border-style:solid;border-width:1px;text-align:center;height:60px;width:234px;overflow:hidden"><span class="u-display-block">Brought to you by:</span><span id="library-banner-text" class="branded"><a target="_blank" href="http://gmitlib.gmit.ie/" rel="nofollow">GMIT Library</a></span></div></div></div><div class="mobile-library-banner move-top move-center u-bg-grey1 u-padding-xxl-ver u-hide-from-lg u-display-block" id="mobile-library-banner" style="width:100%;position:relative"><div class="customer-banner-container text-xs move-middle move-center"><div class="text-customer-banner text-xs u-bg-white u-fill-black u-clr-black" style="border-color:#c9c9c9;border-style:solid;border-width:1px;text-align:center;height:60px;width:234px;overflow:hidden"><span class="u-display-block">Brought to you by:</span><span id="library-banner-text" class="branded"><a target="_blank" href="http://gmitlib.gmit.ie/" rel="nofollow">GMIT Library</a></span></div></div><button id="close-library-banner" class="button button-anchor move-top move-right u-padding-s-right u-clr-grey7" type="button"><span class="button-text"><span class="u-hide-visually">Close</span></span><svg focusable="false" viewBox="0 0 70 128" width="13.125" height="24" class="icon icon-cross"><path d="m68.94 36.12l-6.94-7.12-27 27-27-27-7 7 27 27-27 27 7 7 27-27 27 27 7-7-27-27 26.94-26.88"></path></svg></button></div></div></nav></div><div class="u-hide-from-md move-right"><ul><li style="list-style:none;display:inline-block;margin-top:32px" class="u-margin-s-hor u-margin-l-hor-from-sm"><div class="hamburger-button"><button class="button-link button-link-primary" aria-label="Mobile menu" aria-expanded="false" type="button"><svg viewBox="0 0 40 18" width="40" height="18" y="52"><path d="M0 16h40v2H0zm0-8h40v2H0zm0-8h40v2H0z"></path></svg><span class="button-link-text"></span></button></div></li></ul><div style="bottom:0;left:0;position:fixed;top:0;width:100%;z-index:70;opacity:.8" class="u-bg-grey1 u-display-none"></div><div id="mobile-menu" style="overflow:auto;position:fixed;width:288px;right:0;top:0;z-index:1000;height:100%" aria-label="Mobile menu" class="u-bg-grey7 u-clr-grey1 u-display-none" role="navigation"><div class="u-bg-black panel-s"><div style="text-overflow:ellipsis;overflow:hidden;white-space:nowrap;display:inline-block;width:195px"><span class="u-fill-grey5"><svg focusable="false" viewBox="0 0 106 128" height="32" width="27" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg></span><span class="u-margin-xs-left u-clr-grey5 text-s" role="presentation" style="vertical-align:bottom">My account</span></div></div><div class="u-bg-grey8 panel-s"><h3 class="text-xs u-clr-grey4 u-margin-xs-bottom"><span>Galway Mayo Institute of Technology, Shibboleth (401002)</span></h3><ul><li style="list-style:none"><a class="anchor journals-and-books-link u-padding-xs-top anchor-has-inherit-color" href="https://www.sciencedirect.com/browse/journals-and-books" style="border-bottom:" id="mobile-journals-and-books-link"><span class="anchor-text">Journals &amp; Books</span></a></li></ul></div><div class="panel-s u-bg-grey7"><ul class="text-xs"></ul><div><a class="anchor u-clr-grey1 u-fill-grey1 u-h5" href="#" aria-expanded="false" title="Details &amp; settings"><span class="anchor-text">Details &amp; settings<svg focusable="false" viewBox="0 0 92 128" style="float:right" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></span></a></div><ul class="u-margin-s-top text-s"><li style="list-style:none"><a class="anchor anchor-has-inherit-color" href="https://www.sciencedirect.com/user/logout?targetURL=%2Fscience%2Farticle%2Fpii%2FS0003687017302867" id="mobile-sign-in-out-link" rel="nofollow"><span class="anchor-text">Sign out</span></a></li><span><a class="anchor u-padding-xs-top text-s help-link anchor-has-inherit-color" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" id="mobile-help" title="Help" aria-label="Help" target="_blank"><span class="anchor-text">Help</span></a></span></ul></div></div></div></div></div></div><div class="Article" id="mathjax-container"><div class="sticky-outer-wrapper"><div class="sticky-inner-wrapper" style="position: relative; z-index: 1; transform: translate3d(0px, 0px, 0px);"><div class="Toolbar" role="region" aria-label="download options and search"><div class="toolbar-container"><div class="u-show-from-lg col-lg-6">&nbsp;</div><div class="buttons pull-left pad-left"><button class="button show-toc-button button-anchor u-hide-from-lg u-margin-s-right" type="button"><svg focusable="false" viewBox="0 0 104 128" width="19.5" height="24" class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg><span class="button-text"><span>Outline</span></span></button><div class="PdfDownloadButton"><a class="anchor u-margin-s-right" href="https://www.sciencedirect.com/science/article/pii/S0003687017302867/pdfft?md5=af42c555a80c115353af2b16ffca109c&amp;pid=1-s2.0-S0003687017302867-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text"><span class="pdf-download-label u-show-inline-from-lg">Download PDF</span><span class="pdf-download-label-short u-hide-from-lg">Download</span></span></a></div><div class="Social" id="social"><div class="popover social-popover" id="social-popover" aria-label="Share article on social media"><div id="popover-trigger-social-popover"><button class="button button-anchor" role="button" aria-haspopup="true" aria-expanded="false" type="button"><span class="button-text">Share</span></button></div></div></div><div class="ExportCitation" role="region" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button button-anchor" role="button" aria-haspopup="true" aria-expanded="false" type="button"><span class="button-text">Export</span></button></div></div></div></div><div class="quick-search-container pull-right pad-right u-show-from-md"><form id="quick-search" class="QuickSearch u-margin-xs-right" action="/search/advanced" method="get"><input class="query" aria-label="Search ScienceDirect" name="qs" placeholder="Search ScienceDirect" type="text"><button class="button button-primary" type="submit" aria-label="Submit search"><span class="button-text"><svg focusable="false" viewBox="0 0 100 128" height="20" width="18.75" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button><a class="advanced-search-link" href="https://www.sciencedirect.com/search/advanced">Advanced</a><input name="origin" value="article" type="hidden"><input name="zone" value="qSearch" type="hidden"></form></div></div></div></div></div><div class="article-wrapper grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang="en"><div class="Outline" id="toc-outline"><h2>Outline</h2><ol class="u-padding-s-bottom"><li><a href="#abs0020" title="Highlights">Highlights</a></li><li><a href="#abs0010" title="Abstract">Abstract</a></li><li><a href="#abs0015" title="Graphical abstract">Graphical abstract</a></li><li><a href="#kwrds0010" title="Keywords">Keywords</a></li><li><a href="#sec1" title="1. Introduction">1. Introduction</a></li><li><a href="#sec2" title="2. Methods">2. Methods</a></li><li><a href="#sec3" title="3. Results">3. Results</a></li><li><a href="#sec4" title="4. Discussion">4. Discussion</a></li><li><a href="#sec5" title="5. Conclusion">5. Conclusion</a></li><li><a href="#ack0010" title="Acknowledgements">Acknowledgements</a></li><li><a href="#cebib0010" title="References">References</a></li></ol><button class="button button-anchor" type="button"><span class="button-text">Show full outline</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2>Figures (8)</h2><ol><li><a href="#undfig1"><div><img alt="Unlabelled figure" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-fx1.sml" style="max-width: 219px; max-height: 109px;"></div></a></li><li><a href="#fig1"><div><img alt="Fig. 1. Illustration of the experiment layout presenting the relative distances ofâ€¦" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr1.sml" style="max-width: 219px; max-height: 107px;"></div></a></li><li><a href="#fig2"><div><img alt="Fig. 2. Experiment Task" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr2.sml" style="max-width: 219px; max-height: 159px;"></div></a></li><li><a href="#fig3"><div><img alt="Fig. 3. Parallax settings" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr3.sml" style="max-width: 219px; max-height: 100px;"></div></a></li><li><a href="#fig4"><div><img alt="Fig. 4. Effect of parallax on fixation duration" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr4.sml" style="max-width: 219px; max-height: 96px;"></div></a></li><li><a href="#fig5"><div><img alt="Fig. 5. Effect of parallax on number of fixations" src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr5.sml" style="max-width: 219px; max-height: 97px;"></div></a></li></ol><button class="button button-anchor" type="button"><span class="button-text">Show all figures</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role="main" lang="en"><noscript>JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.</noscript><div class="Publication" id="publication"><div class="publication-brand u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/00036870"><img class="publication-brand-image" src="elsevier-non-solus.png" alt="Elsevier"></a></div><div class="publication-volume u-text-center"><h2 class="publication-title" id="publication-title"><a class="publication-title-link" title="Go to Applied Ergonomics on ScienceDirect" href="https://www.sciencedirect.com/science/journal/00036870">Applied Ergonomics</a></h2><div class="text-xs"><a title="Go to table of contents for this volume/issue" href="https://www.sciencedirect.com/science/journal/00036870/69/supp/C">Volume 69</a>, <!-- -->May 2018<!-- -->, Pages 10-16</div></div><div class="publication-cover u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/00036870/69/supp/C"><img class="publication-cover-image" src="1-s2.gif" alt="Applied Ergonomics"></a></div></div><h1 class="Head"><span class="title-text">The effect of parallax on eye fixation parameter in projection-based stereoscopic displays</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><a class="author size-m workspace-trigger" name="bau1" href="#%21"><span class="content"><span class="text given-name">Chiuhsiang Joe</span><span class="text surname">Lin</span><svg focusable="false" viewBox="0 0 106 128" width="19.875" height="24" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable="false" viewBox="0 0 102 128" width="19.125" height="24" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name="bau2" href="#%21"><span class="content"><span class="text given-name">Retno</span><span class="text surname">Widyaningrum</span></span></a></div></div></div><button class="show-hide-details" type="button"><svg viewBox="0 0 9 9" class="icon-expand"><path d="M5 7H4V5H2V4h2V2h1v2h2v1H5z"></path><path d="M0 0v9h9V0zm1 1h7v7H1z"></path></svg>Show more</button></div><div class="DoiLink" id="doi-link"><a class="doi" href="https://doi.org/10.1016/j.apergo.2017.12.020" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi.org/10.1016/j.apergo.2017.12.020</a><a class="rights-and-content" target="_blank" rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0003687017302867&amp;orderBeanReset=true">Get rights and content</a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts" id="abstracts"><div class="abstract author-highlights" id="abs0020" lang="en"><h2 class="section-title">Highlights</h2><div id="abssec0020"><p id="abspara0020"><dl class="list"><dt class="list-label">â€¢</dt><dd class="list-description"><p id="p0010">Eye movement was analyzed in three different levels of object depth (parallax) in a projection-based stereoscopic display.</p></dd><dt class="list-label">â€¢</dt><dd class="list-description"><p id="p0015">Significant effects of parallax were found.</p></dd><dt class="list-label">â€¢</dt><dd class="list-description"><p id="p0020">Virtual objects at the screen results in better accuracy.</p></dd><dt class="list-label">â€¢</dt><dd class="list-description"><p id="p0025">The result contributes to designing effective interactive tasks where different targets appear in the stereoscopic display.</p></dd></dl></p></div></div><div class="abstract author" id="abs0010" lang="en"><h2 class="section-title">Abstract</h2><div id="abssec0010"><p id="abspara0010">The
 promising technology of stereoscopic displays is interesting to explore
 because 3D virtual applications are widely known. Thus, this study 
investigated the effect of parallax on eye fixation in stereoscopic 
displays. The experiment was conducted in three different levels of 
parallax, in which virtual balls were projected at the screen, at 20â€¯cm 
and 50â€¯cm in front the screen. The two important findings of this study 
are that parallax has significant effects on fixation duration, time to 
first fixation, number of fixations, and accuracy. The participant had 
more accurate fixations, fewer fixations, shorter fixation durations, 
and shorter times to first fixation when the virtual ball was projected 
at the screen than when it was projected at the other two levels of 
parallax.</p></div></div><div class="abstract graphical" id="abs0015"><h2 class="section-title">Graphical abstract</h2><div id="abssec0015"><p id="abspara0015">While
 a person is looking at targets displayed in a virtual environment (a 
stereoscopic display), the eye movement is recorded and analysed, and 
the interaction performance using a clicking task is examined.<span class="display"><figure class="figure" id="undfig1"><span><img src="1-s2.jpg" alt="Image 1" height="200"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-fx1_lrg.jpg" target="_blank" download="" title="Download high-res image (134KB)"><span class="anchor-text">Download high-res image (134KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-fx1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span></figure></span></p></div></div></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href="https://www.sciencedirect.com/science/article/pii/S0003687017302855"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text"><strong>Previous </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary" href="https://www.sciencedirect.com/science/article/pii/S0003687018300012"><span class="button-alternative-text"><strong>Next </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords"><div id="kwrds0010" class="keywords-section"><h2 class="section-title">Keywords</h2><div id="kwrd0010" class="keyword"><span>Depth perception</span></div><div id="kwrd0015" class="keyword"><span>Eye fixation</span></div><div id="kwrd0020" class="keyword"><span>3D tasks</span></div></div></div><div class="Body" id="body"><div><section id="sec1"><h2 id="sectitle0025">1. Introduction</h2><p id="p0030">Nowadays,
 virtual reality (VR) is becoming widely popular, and the promising 
technology of VR has been applied in training for laparoscopic surgery (<a name="bbib15" href="#bib15" class="workspace-trigger">Hart and Karthigasu, 2007</a>), virtual rehabilitation therapy (<a name="bbib6" href="#bib6" class="workspace-trigger">Burdea, 2002</a>), balance exercise programmes for traumatic brain injury (<a name="bbib45" href="#bib45" class="workspace-trigger">Thornton et al., 2005</a>), training for autistic spectrum disorder (<a name="bbib37" href="#bib37" class="workspace-trigger">Parsons and Mitchell, 2002</a>), training in the automotive industry (<a name="bbib23" href="#bib23" class="workspace-trigger">Lawson et al., 2016</a>), manufacturing process simulations (<a name="bbib33" href="#bib33" class="workspace-trigger">Mujber et al., 2004</a>), wheelchair driving simulator (<a name="bbib1" href="#bib1" class="workspace-trigger">Alshaer et al., 2017</a>) and education (<a name="bbib3" href="#bib3" class="workspace-trigger">Bell and Fogler, 1995</a>).
 A VR system provides interactive computer graphics that allow users to 
experience personal 3D viewing and interact with objects in a virtual 
environment (VE) (<a name="bbib8" href="#bib8" class="workspace-trigger">Czernuszenko et al., 1997</a>, <a name="bbib42" href="#bib42" class="workspace-trigger">Sharples et al., 2008</a>). The hardware devices required to achieve a 3D VE include a computer, a device display, and a handheld input device (<a name="bbib27" href="#bib27" class="workspace-trigger">Lin et al., 2015a</a>).
 In the early 1990s, the head mounted display (HMD) was the leading 
device display screen that allowed users to experience an immersive 
presence in VEs (<a name="bbib42" href="#bib42" class="workspace-trigger">Sharples et al., 2008</a>).
 Moreover, users can have their own personal displays to interact and 
experience virtual targets because an HMD can effectively block out the 
real environment (<a name="bbib31" href="#bib31" class="workspace-trigger">Mcneill et al., 2004</a>).
 However, HMDs have some limitations and can be invasive to users. HMDs 
are worn on the head, and the weight is carried by the neck (<a name="bbib8" href="#bib8" class="workspace-trigger">Czernuszenko et al., 1997</a>). This weight can cause modification of neck posture and increase stress on the musculoskeletal system of the head and neck (<a name="bbib32" href="#bib32" class="workspace-trigger">Mon-williams et al., 1995</a>). Moreover, users who wear an HMD may experience symptoms of nausea, dizziness, vomiting, and visual problems (<a name="bbib32" href="#bib32" class="workspace-trigger">Mon-williams et al., 1995</a>, <a name="bbib19" href="#bib19" class="workspace-trigger">Knight and Baber, 2007</a>). To provide a wide field of view, HMDs require non-linear optic, which cause distortion (<a name="bbib8" href="#bib8" class="workspace-trigger">Czernuszenko et al., 1997</a>). Furthermore, HMDs allow users to perceive virtual objects in positive and zero parallax as most close to user's eyes (<a name="bbib51" href="#bib51" class="workspace-trigger">Zhou et al., 2008</a>).</p><p id="p0035">As
 another option, a projection-based system in VR has been developed in 
recent years. Projection-based VR uses wide, large, multi-touch, tracked
 hand-held display, and a fixed screen display in a relative distance 
from the user (<a name="bbib4" href="#bib4" class="workspace-trigger">Benko et al., 2004</a>).
 Such a projection system provides several advantages. First, it allows 
for multiple users to share and communicate with each other about the 3D
 environment by wearing 3D glasses (<a name="bbib42" href="#bib42" class="workspace-trigger">Sharples et al., 2008</a>).
 Second, a projection-based system can minimize the stress on the 
musculoskeletal system because users simply wear lightweight (3.3 oz) 3D
 glasses (<a name="bbib8" href="#bib8" class="workspace-trigger">Czernuszenko et al., 1997</a>).
 In that condition, users can comfortably view 3D images and interact 
with the virtual environment. Projection display also produces a 
satisfying interaction feeling of augmentation in augmented reality (AR)
 (<a name="bbib51" href="#bib51" class="workspace-trigger">Zhou et al., 2008</a>). AR systems incorporate real and virtual objects in a real time (<a name="bbib2" href="#bib2" class="workspace-trigger">Azuma et al., 2001</a>).
 In a projection system, a virtual object appears in negative parallax 
(in front of the projection plane) and positive parallax (<a name="bbib38" href="#bib38" class="workspace-trigger">Petkov, 2012</a>);
 therefore, a user can obtain satisfactory depth perception and 
interaction with virtual objects. However, such a system also has 
drawbacks; a projection system lacks mobility because the set up of 
projection stay in fixed position (<a name="bbib51" href="#bib51" class="workspace-trigger">Zhou et al., 2008</a>), and it cannot rotate the whole virtual world when the user rotates his or her head (<a name="bbib27" href="#bib27" class="workspace-trigger">Lin et al., 2015a</a>).</p><p id="p0040">In
 this study, we used projection-based stereoscopic 3D displays. The 
stereoscopic 3D display is one technique for enhancing the illusion of 
depth in an image. The principle behind generating 3D images requires 
the control of parameters, especially parallax. Parallax is the 
horizontal display disparity of two images between the left and right 
eyes to create 3D images (<a name="bbib44" href="#bib44" class="workspace-trigger">Smith et al., 2012</a>, <a name="bbib22" href="#bib22" class="workspace-trigger">Lang et al., 2010</a>).
 The eyesâ€™ axes of vision cross at the particular point where the 
virtual object is located, and the convergence of the eyes creates an 
illusion of depth (<a name="bbib52" href="#bib52" class="workspace-trigger">Seigle, 2009</a>). Human perception requires the left and right eye to see difference perspective images in the same scene (<a name="bbib46" href="#bib46" class="workspace-trigger">Valkov et al., 2011</a>).
 To perceive 3D image, with the left and right images providing 
information, the brain must be able to integrate the depths from the 
projection of two images into a single three-dimensional object (<a name="bbib24" href="#bib24" class="workspace-trigger">Lebreton et al., 2012</a>). Parallax and depth perception have important rules for creating 3D images in a virtual environment.</p><p id="p0045">However,
 the binocular depth perception relation in stereoscopic displays may 
cause visual problems, and parallax might be the underlying cause. The 
problems may be visual discomfort (<a name="bbib21" href="#bib21" class="workspace-trigger">Lambooij et al., 2009</a>), asthenopia or eye without strength (<a name="bbib29" href="#bib29" class="workspace-trigger">Mackenzie, 1843</a>), eyestrain (<a name="bbib7" href="#bib7" class="workspace-trigger">Council, 1983</a>), accommodation and vergence mismatch (<a name="bbib12" href="#bib12" class="workspace-trigger">Emoto et al., 2005</a>, <a name="bbib17" href="#bib17" class="workspace-trigger">Hoffman et al., 2008</a>, <a name="bbib50" href="#bib50" class="workspace-trigger">Wann et al., 1995</a>, <a name="bbib34" href="#bib34" class="workspace-trigger">Okada et al., 2006</a>). Moreover, parallax may also caused inaccuracy of distance judgment (<a name="bbib28" href="#bib28" class="workspace-trigger">Lin et al., 2015b</a>, <a name="bbib26" href="#bib26" class="workspace-trigger">Lin and Woldegiorgis, 2017</a>). Therefore, it is worthwhile to investigate the effects of parallax in a virtual environment.</p><p id="p0050">Investigation
 of the effects of parallax requires discovery of the eye parameters in 
stereoscopic displays. Eye tracking technology is powerful device for 
investigating the effects of parallax and depth perception because eye 
movement data provide evidence of visual attention as fundamental system
 in visual perception (<a name="bbib47" href="#bib47" class="workspace-trigger">Von Helmholz, 1897</a>).
 Recording eye movement data can allow detection of the visual attention
 path of a participant, and then it can be derived that the human 
perceived the object (<a name="bbib11" href="#bib11" class="workspace-trigger">Duchowski, 2007</a>).
 Eye tracking technology captures what the user is looking at. Eye 
tracking detects and tracks the movement of the eyes and records the eye
 points and gaze points. In eye tracking analysis, an algorithm is used 
to classify data as eye fixation data or saccade (<a name="bbib43" href="#bib43" class="workspace-trigger">Shic et al., 2008</a>).
 The application of eye tracking technology allows our eyes to control a
 device as naturally as the movement of our eyes, so eye tracking 
technology has been used in many disciplines of research, such as 
measuring software screen complexity (<a name="bbib13" href="#bib13" class="workspace-trigger">Goldberg, 2014</a>), web page viewing behaviour (<a name="bbib36" href="#bib36" class="workspace-trigger">Pan et al., 2004</a>), eye pointing performance (<a name="bbib25" href="#bib25" class="workspace-trigger">Lin and Widyaningrum, 2016</a>),
 market research and advertising testing, eye control for accessibility,
 psychology and vision research, medical research, diagnostics and 
rehabilitation, and gaze interaction and car assistant systems (<a name="bbib10" href="#bib10" class="workspace-trigger">Drewes, 2010</a>). Hence, it is very helpful to visualize a participant's eye movement with eye tracking technology (<a name="bbib9" href="#bib9" class="workspace-trigger">Deutsch and Deutsch, 1963</a>).</p><p id="p0055">In
 eye tracker data, eye fixation data are the fundamental data used to 
evaluate eye performance and behaviour. Eye fixation is fascinating to 
explore because a cognitive process allows the viewer to see something 
interesting in the eye fixation process (<a name="bbib5" href="#bib5" class="workspace-trigger">Blignaut, 2009</a>). The oculomotor definition of eye fixation is that the eyes remain in the current position (<a name="bbib18" href="#bib18" class="workspace-trigger">Holmqvist et al., 2011</a>).
 A previous study examined the eye fixations of users in accomplishing a
 task and identified the number, position, and duration of fixations in 
screen displays. The study was conducted to measure eye fixation 
(number, number per line, rate, duration, and words per fixation) as a 
function of character and line spacing in a reading task (<a name="bbib20" href="#bib20" class="workspace-trigger">Kolers et al., 1981</a>).
 The result showed that more fixations per line and fewer fixations per 
word were associated with more tightly-grouped, singled-spaced material.
 <a name="bbib14" href="#bib14" class="workspace-trigger">Goldberg and Kotval (1999)</a>
 presented an introduction and a framework of eye movement analysis 
techniques. That study recruited 12 subjects to experience good and poor
 software interfaces and measured the number of fixations, fixation 
duration, fixation/saccade ratio, and other measurements. The number of 
fixations is related to the number of components that the user is 
required to process to select the target. Participants made more 
fixations when they experienced bad interface design. Furthermore, 
fixation duration was required by the participants to interpret or 
relate the components represented in the interface. The duration of a 
single fixation on targets was dependent on the interface layout. <a name="bbib13" href="#bib13" class="workspace-trigger">Goldberg (2014)</a>
 conducted a study to measure the complexity of software screens by 
relating eye tracking, emotional valence, and subjective ratings. In 
that study, participants were asked to complete 25 tasks on screen pages
 designed with various combinations of page category, gradient, font, 
and font size combination. The results showed that participantsâ€™ time to
 first fixation was longer when the larger fonts required searching a 
larger search area, and longer completion times caused a greater number 
of fixations. Therefore, longer fixation durations were associated with 
confusion or difficulty in processing tasks. Those studies showed that 
eye fixation was associated with efficiency in performance. Eye fixation
 is important to engineers designing effective displays to improve 
usability issue.</p><p id="p0060">However, as explained above, most of 
the literature about eye fixation has been conducted using 2D or screen 
displays. The analysis of eye fixation has not considered depth 
perception in accomplishing a task. Therefore, in this study, we 
investigated the effect of parallax on eye fixation parameter, 
especially number of fixations, time to first fixation, fixation 
duration, and accuracy in stereoscopic displays. We predicted that the 
fixation duration, number of fixations, time to first fixation, and 
accuracy would be affected by parallax due to the limitations of the 
eyes in a 3D visual environment.</p></section><section id="sec2"><h2 id="sectitle0030">2. Methods</h2><p id="p0065">The
 aim of this study was to investigate the effects of parallax on eye 
fixation in projection-based stereoscopic displays. Participants were 
asked to perform a pointing task in which a mouse with a 3D cursor and 
their eyes were used to point to a concentric circle of virtual balls. 
An eye tracker recorded the eye movements and eye fixations of the 
participants as they used hand movements to move the 3D cursor to point 
to the virtual ball. In this study, three different levels of parallax 
was used to examine the differences in depth perception of a 3D virtual 
ball projected level with the screen (at the screen), 20â€¯cm in front of 
the screen, or 50â€¯cm in front of the screen. The participants 
accomplished the trials in randomized order for each level of parallax.</p><section id="sec2.1"><h3 id="sectitle0035">2.1. Participants</h3><p id="p0070">Ten
 graduate students at National Taiwan University of Science and 
Technology participated in this study. Their mean age was 25 years with a
 standard deviation of four. All participants had normal or corrected to
 normal visual acuity (1.0 in decimal units). The participants were 
volunteers and were not given any compensation for performing the 
stereoscopic task. The study was approved by the ethical guidelines of 
the Research Ethics Committee of National Taiwan University. 
Participants completed consent forms before performing the task.</p></section><section id="sec2.2"><h3 id="sectitle0040">2.2. Apparatus and tools</h3><p id="p0075">The
 Tobii X2-60 eye tracking system, which has a 60â€¯Hz sampling rate, was 
used to record the movements of participantsâ€™ eyes. An I-VT fixation 
filter was used to filter out the raw eye movement data with a 
30Â°/second velocity threshold (<a name="bbib41" href="#bib41" class="workspace-trigger">Salvucci and Goldberg, 2000</a>).
 The fixation filter Tobii Studio version 3.3.2 software was used for 
calibration, testing, and data analysis. Eye movement data were exported
 for further data processing and statistical analysis. To perceive the 
stereoscopic 3D environment, the participant wore a pair of ViewSonic 3D
 glasses (PDF-250) integrated with a 3D vision IR Emitter from NVIDIA 
and a 3D ViewSonic (PJD6251) projector. The projection screen was 
143â€¯cmâ€¯Ã—â€¯108â€¯cm. The virtual ball was drawn using the Unity 3D platform 
(version 4.3.4) run on an Asus Windows Core i5 personal computer. A 
Logitech C-920 webcam integrated with Tobii studio was used to record 
the eye movement data from the screen display.</p><div><p id="p0080">An illustration of the experimental layout is presented in <a name="bfig1" href="#fig1" class="workspace-trigger">Fig. 1</a>.
 The participant was seated fronto-parallel to the screen. The distance 
between the participant and the screen was 181â€¯cm. The projector was 
placed in front of the participant at a distance of 89â€¯cm from the 
screen. The Tobii eye tracker was placed 64â€¯cm in front of the 
participant, and the web camera was placed behind the participant at a 
distance of 200â€¯cm from the screen. All of the devices were kept fixed 
and marked with adhesive tape to maintain consistency of the relative 
distances to the participant. The participant performed the task in a 
dark room (3.6â€¯mâ€¯Ã—â€¯3.2â€¯m x 2.5â€¯m) covered by black curtains to prevent 
the light and to create a good quality stereoscopic environment.</p><figure class="figure" id="fig1"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr1.jpg" alt="Fig. 1" aria-describedby="cap0010" height="306"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (202KB)"><span class="anchor-text">Download high-res image (202KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0010"><p id="fspara0010"><span class="label">Fig. 1</span>. Illustration of the experiment layout presenting the relative distances of apparatus to the participant.</p></span></div></figure></div></section><section id="sec2.3"><h3 id="sectitle0045">2.3. Experiment procedures</h3><p id="p0085">First,
 participants completed a participant consent form that described the 
purpose of the study, instructions on the experimental tasks, experiment
 procedure, and confidentiality of the data of the participant. After 
that, the participant sat on a chair and wore the 3D glasses. In the 
beginning of the experiment, the observer performed a calibration 
procedure and asked the participant to look at the red calibration dot 
as precisely as they could until the red calibration dot disappeared. 
The Tobii studio regular calibration process shows five calibration dots
 to check the accuracy and performance of the Tobii eye tracker. The 
experiment began when the quality of the calibration was excellent.</p><div><p id="p0090">In
 this study, participants were instructed to fixate their eyes on a 
virtual cube and click it using a virtual 3D mouse to start the task 
(see <a name="bfig2" href="#fig2" class="workspace-trigger">Fig. 2</a>).
 After the virtual cube was clicked, a virtual red ball appeared. The 
participant was to identify the target and click the virtual target as 
fast and as accurately as possible. The virtual balls were arranged in 
concentric circles with three different levels of parallax. In each 
trial, the participant was to click the virtual ball while the Tobii eye
 tracker simultaneously recorded the participant's eye gaze movement and
 eye fixation point. The total time per participant, including 
completion of the consent form, instructions, the calibration process, 
the experiment, and breaks, was 60â€¯min.</p><figure class="figure" id="fig2"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr2.jpg" alt="Fig. 2" aria-describedby="cap0015" height="389"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (141KB)"><span class="anchor-text">Download high-res image (141KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0015"><p id="fspara0015"><span class="label">Fig. 2</span>. 
Experiment Task. (A) A 3D cube was placed in the centre as the initial 
eye fixation before the participant started the task. The observer asked
 the participant to start the task by clicking the virtual cube while 
the eye tracker captured the participant's eye gaze. (B) The red ball 
(top) would appear when the participant clicked the virtual cube. (C) 
The participant was required to click the red ball and the ball colour 
would change to white (top), after which the next red ball (bottom) 
would appear. (D) Each trial had12 balls and the participant required to
 click all the virtual balls. (For interpretation of the references to 
colour in this figure legend, the reader is referred to the Web version 
of this article.)</p></span></div></figure></div></section><section id="sec2.4"><h3 id="sectitle0050">2.4. Experimental design</h3><div><p id="p0095">The
 independent variable in this study was parallax, which is the setting 
to define the position of the virtual ball relative to the position of 
the fixed screen and participant's eyes (<a name="bbib25" href="#bib25" class="workspace-trigger">Lin and Widyaningrum, 2016</a>, <a name="bbib28" href="#bib28" class="workspace-trigger">Lin et al., 2015b</a>).
 In this study, we developed zero parallax (at the screen) and negative 
parallax (20 and 50â€¯cm in front of the screen) and the binocular 
disparity range was chosen to minimize the effect of visual fatigue (<a name="bbib25" href="#bib25" class="workspace-trigger">Lin and Widyaningrum, 2016</a>).
 In order to provide 3D image perception but to minimize visual fatigue,
 we set the binocular disparity to be less than the inter-pupillary 
distance (IPD) value (6.5â€¯cm), which determines the difference of the 
projected image locations or coordinates of an object seen by the left 
and the right eyes. Since the displayed area had a horizontal dimension 
of 108â€¯cm, which is equivalent to the horizontal visual angle of 73.2Â°. 
The displayed ball size at each parallax was calculated and changed 
accordingly to remain at the same visual angle for different parallax 
conditions. <a name="bfig3" href="#fig3" class="workspace-trigger">Fig. 3</a> presents the parallax settings of the virtual target arrangement and binocular disparity for each different level of parallax.</p><figure class="figure" id="fig3"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr3.jpg" alt="Fig. 3" aria-describedby="cap0020" height="324"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (278KB)"><span class="anchor-text">Download high-res image (278KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0020"><p id="fspara0020"><span class="label">Fig. 3</span>. 
Parallax settings. (A) Top view of the virtual ball arrangement in three
 different levels of parallax. The participant would see the virtual 
ball at the screen, 20â€¯cm in front of the screen, and 50â€¯cm in front of 
the screen. (B) Binocular disparity for each parallax level.</p></span></div></figure></div><p id="p0100">The dependent variables were number of fixations, time to first fixation, fixation duration (<a name="bbib13" href="#bib13" class="workspace-trigger">Goldberg, 2014</a>),
 and accuracy. Number of fixations was the number of eye fixations 
starting from the time the hand clicked the virtual ball from origin to 
destination target. Time to first fixation was elapsed time from the 
beginning of the task until the first fixation on the virtual cube. 
Fixation duration was the average duration of fixations made by the 
participant to click the virtual ball from the origin to destination 
target. Accuracy was measured by the distance between the recorded 
fixation locations and the actual location of the projection of the 
image (<a name="bbib35" href="#bib35" class="workspace-trigger">Ooms et al., 2015</a>).
 Eye fixation position (EFp) was determined by eye gaze algorithm 
selection to fulfil the criteria of eye fixation parameters (<a name="bbib25" href="#bib25" class="workspace-trigger">Lin and Widyaningrum, 2016</a>).
 The candidates of eye fixation points were the fixation points with 
timestamps between two mouse clicks. Hand click time data were collected
 for matching with the timestamp data from the eye tracker data to 
facilitate the classification of eye fixations. <a name="bbib30" href="#bib30" class="workspace-trigger">Manor and Gordon (2003)</a>
 stated that the recommended minimum threshold for fixation duration was
 0.1â€“0.2â€¯s. In this study, the minimum fixation duration was 0.14â€¯s, 
which was considered as the intention to click on the ball target. The 
eye fixation point was chosen based on the closest eye fixation location
 to the ball, which was determined by a circle with a diameter 1.5 times
 the virtual ball target width. We intended to choose the closest 
fixation point to minimize inaccuracy in predicting the location of the 
eye gaze on the ball position.</p><p id="p0105">The accuracy judgement was calculated with the following formula:<span class="display"><div id="fd1" class="formula"><span class="label">(1)</span><span class="math"><math><mrow is="true"><mtext is="true">Accuracy</mtext><mi is="true" mathvariant="normal">=</mi><mrow is="true"><mo is="true">(</mo><mrow is="true"><mn is="true">1</mn><mo is="true">âˆ’</mo><mrow is="true"><mo is="true">|</mo><mrow is="true"><mfrac is="true"><mrow is="true"><mtext is="true">EFp</mtext><mo is="true">âˆ’</mo><mtext is="true">IPp</mtext></mrow><mrow is="true"><mtext is="true">IPp</mtext></mrow></mfrac></mrow><mo is="true">|</mo></mrow></mrow><mo is="true">)</mo></mrow></mrow></math></span></div></span>Where.<dl class="list"><dd class="list-description"><p id="p0110">EFpâ€¯=â€¯Eye fixation position</p></dd><dd class="list-description"><p id="p0115">IPpâ€¯=â€¯Image projection position</p></dd></dl></p><p id="p0120">Eye
 fixation positions were recorded by Tobii studio in pixels, and then 
the coordinate positions were converted into mm. Image projection 
position was measured from the location of the projection image to the 
screen in mm. The x axis was measured from left to right, and the y axis
 from the bottom to the top.</p></section></section><section id="sec3"><h2 id="sectitle0055">3. Results</h2><p id="p0125">This
 section presents the results of one way repeated-measures ANOVA for the
 three levels of parallax on each dependent variable: fixation duration,
 number of fixations, time to first fixation, and accuracy. Post hoc 
tests were conducted using Tukey's HSD (Î±â€¯=â€¯0.05) when the ANOVA results
 showed significant effects.</p><section id="sec3.1"><h3 id="sectitle0060">3.1. Fixation duration</h3><div><p id="p0130">The
 mean and standard deviations of fixation durations for the three levels
 of parallax, at the screen, and 20 and 50â€¯cm in front of the screen, 
were 0.43â€¯s (sdâ€¯=â€¯0.09), 0.68â€¯s (sdâ€¯=â€¯0.13), and 0.72â€¯s (sdâ€¯=â€¯0.25). The
 fixation duration for parallax of 50â€¯cm in front of the screen was the 
longest, and that for parallax at the screen was the shortest. The 
result of repeated measures ANOVA indicated significant interactions 
between parallax and fixation duration (F<sub>2,18</sub>â€¯=â€¯9.25, pâ€¯=â€¯.002). <a name="bfig4" href="#fig4" class="workspace-trigger">Fig. 4</a>
 shows the interaction of parallax and fixation duration. Post hoc 
analysis results classified the independent variables into two groups. 
The test results showed that the fixation duration of parallax at the 
screen was significantly shorter than those of parallax of 20 and 50â€¯cm 
in front of the screen. The post hoc test was not significant between 
20â€¯cm and 50â€¯cm in front of the screen. Fixation duration increased with
 parallax in general, and it increased rapidly when the virtual ball was
 20 and 50â€¯cm in front of the screen as compared to at the screen.</p><figure class="figure" id="fig4"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr4.jpg" alt="Fig. 4" aria-describedby="cap0025" height="223"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (101KB)"><span class="anchor-text">Download high-res image (101KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0025"><p id="fspara0025"><span class="label">Fig. 4</span>. 
Effect of parallax on fixation duration. Participants required longer 
fixation durations when the virtual ball was projected close to the 
participants' eyes. The error bar presents the standard error of mean.</p></span></div></figure></div></section><section id="sec3.2"><h3 id="sectitle0065">3.2. Number of fixations</h3><div><p id="p0135">The main effect of parallax (F<sub>2,18</sub>â€¯=â€¯118.83,
 pâ€¯=â€¯.000) was significant for number of fixations. The average number 
of fixations was 3.09 fixations (sdâ€¯=â€¯0.33) for parallax at the screen, 
followed by parallax of 20â€¯cm in front of the screen, 1.29 fixations 
(sdâ€¯=â€¯0.17); and then parallax of 50â€¯cm in front of the screen, 1.24 
fixations (sdâ€¯=â€¯0.19) (See <a name="bfig5" href="#fig5" class="workspace-trigger">Fig. 5</a>).
 Post-hoc Tukey test showed that the number of fixations of parallax at 
the screen differed from those of parallax of 20 and 50â€¯cm in front of 
the screen. There was no significant difference in number of fixations 
when the virtual ball was projected at 20 and 50â€¯cm in front of the 
screen. Number of fixations decreased when the virtual ball was 
projected closer to the participant's eyes.</p><figure class="figure" id="fig5"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr5.jpg" alt="Fig. 5" aria-describedby="cap0030" height="235"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (121KB)"><span class="anchor-text">Download high-res image (121KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0030"><p id="fspara0030"><span class="label">Fig. 5</span>. 
Effect of parallax on number of fixations. A lower fixation number of 
fixations occurred when the virtual ball was projected closer to the 
participant's eyes. The error bar presents the standard error of mean.</p></span></div></figure></div></section><section id="sec3.3"><h3 id="sectitle0070">3.3. Time to first fixation</h3><div><p id="p0140">Compared
 to parallax at the screen, the presence of a virtual ball closer to the
 participant's eyes (parallax 20 and 50â€¯cm in front of the screen) 
increased the time to first fixation from 1.13â€¯s (sdâ€¯=â€¯0.96) to 3.15â€¯s 
(sdâ€¯=â€¯1.5) (see <a name="bfig6" href="#fig6" class="workspace-trigger">Fig. 6</a>). There were significant main effects of parallax and time to first fixation (F<sub>2,18</sub>â€¯=â€¯7.95,
 pâ€¯=â€¯.003). Post hoc analysis using Tukey's method for significance 
indicated that the average time to first fixation was significantly 
shorter in parallax at the screen than in the other two parallax 
conditions. However, there was no significant difference between 
parallax condition 20â€¯cm and 50â€¯cm in front of the screen.</p><figure class="figure" id="fig6"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr6.jpg" alt="Fig. 6" aria-describedby="cap0035" height="221"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (116KB)"><span class="anchor-text">Download high-res image (116KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0035"><p id="fspara0035"><span class="label">Fig. 6</span>. 
Effect of parallax on first time fixation. Participants needed more time
 to fixate on the virtual objects closer to their eyes. The error bar 
presents the standard error of mean.</p></span></div></figure></div></section><section id="sec3.4"><h3 id="sectitle0075">3.4. Accuracy</h3><div><p id="p0145">There was a significant effect of parallax on eye fixation accuracy (F<sub>2,18</sub>â€¯=â€¯5.91,
 pâ€¯=â€¯.016). The highest accuracy of eye fixation was achieved when the 
virtual ball was projected at the screen 0.94 (sdâ€¯=â€¯0.02), followed by 
the virtual ball projected at 20â€¯cm in front of the screen 0.88 
(sdâ€¯=â€¯0.07) and 50â€¯cm in front of the screen 0.85 (sdâ€¯=â€¯0.1). <a name="bfig7" href="#fig7" class="workspace-trigger">Fig. 7</a>
 shows the effect of parallax, accuracy of eye fixation, and standard 
error of mean. The Tukey post hoc result of eye fixation accuracy 
indicated a significant difference between parallax projected at the 
screen and the other parallax conditions. However, there was no 
significant difference between parallax of 20 and 50â€¯cm in front of the 
screen.</p><figure class="figure" id="fig7"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr7.jpg" alt="Fig. 7" aria-describedby="cap0040" height="212"><ol class="links-for-figure"><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (105KB)"><span class="anchor-text">Download high-res image (105KB)</span></a></li><li><a class="anchor download-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0003687017302867-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download full-size image</span></a></li></ol></span><div class="captions"><span id="cap0040"><p id="fspara0040"><span class="label">Fig. 7</span>. 
Effect of parallax; the accuracy was better when the target was 
displayed at the screen than at 20 and 50â€¯cm in front of the screen. The
 error bar presents the standard error of mean.</p></span></div></figure></div></section></section><section id="sec4"><h2 id="sectitle0080">4. Discussion</h2><p id="p0150">The
 present study investigated the effects of parallax on eye fixation 
parameters in projection-based stereoscopic displays. We conducted an 
experiment using a within-subjects design with parallax as the 
independent variable. Parallax was assessed for the effects on the 
fixation duration, number of fixations, time to first fixation, and 
accuracy. In this experiment, the participants were asked to click 
virtual balls arranged in a concentric circle and projected at three 
levels of parallax (at the screen, 20â€¯cm in front of the screen, and 
50â€¯cm in front of the screen). The overall results of one-way repeated 
ANOVA revealed that fixation duration, number of fixations, time to 
first fixation, and accuracy differed significantly among parallax 
conditions.</p><p id="p0155">Fixation durations were longer when the 
virtual ball was 20 and 50â€¯cm in front of the screen than when it was at
 the screen. Longer fixation durations indicated that the participants 
required more attention to process information of the virtual target 
position before they perceived it clearly in the stereoscopic display. 
According to <a name="bbib16" href="#bib16" class="workspace-trigger">Henderson (1992)</a> and <a name="bbib39" href="#bib39" class="workspace-trigger">Reichle et al. (2003)</a>,
 participant attention did not shift until the information processing of
 the virtual target was completed. Therefore, the workload of 
participantsâ€™ eyes increased when they perceived the virtual ball closer
 to their eyes. The fixation duration may have been longer because the 
participants found it difficult to respond to the cognitive processing (<a name="bbib14" href="#bib14" class="workspace-trigger">Goldberg and Kotval, 1999</a>). Cognitive processing requires the brain to extract the relative depth information of a virtual target (<a name="bbib21" href="#bib21" class="workspace-trigger">Lambooij et al., 2009</a>).
 In addition, longer fixation durations were associated with longer 
times to first fixation. Longer times to first fixation occurred when 
participants needed more processing time, such as recognition and 
identification of a virtual ball located closer to their eyes. Longer 
eye adaptation and accommodation processes were required to perceive the
 3D images clearly.</p><p id="p0160">In the number of fixations, 
participants achieved greater number of fixations when the virtual ball 
was projected at the screen than when it was projected 20 and 50â€¯cm in 
front of the screen. <a name="bbib49" href="#bib49" class="workspace-trigger">Wang et al. (2012)</a>
 reported that the number of fixations varied as a function of objects' 
depth using a 3D TV display. In this study, a projection 3D display was 
used and the number of fixations decreased when the virtual target was 
positioned at 20 and 50â€¯cm in front of the screen. Pupillary 
constriction might have occurred when participants were asked to fixate 
on the virtual ball in the near distance (<a name="bbib21" href="#bib21" class="workspace-trigger">Lambooij et al., 2009</a>).
 Moreover, the eyes would accommodate and converge on the position where
 the virtual ball appeared. The left and right eyes would move in 
opposite directions to focus on the location of the virtual ball and 
then bring the image to the fovea. The lenses would maintain the focus 
area on the virtual ball in order to perceive a clear image of it. 
Pupillary constriction, accommodation, and convergence conflict are 
recognized as the ocular near triad (<a name="bbib21" href="#bib21" class="workspace-trigger">Lambooij et al., 2009</a>, <a name="bbib48" href="#bib48" class="workspace-trigger">Von noorden and Campos, 2002</a>).
 It is not clear at the moment how and why the relationship exists 
between the ocular near triad and the depth of the virtual image 
perceived, as it was reflected in the lower number of fixations found 
when the virtual ball's position gets closer to participants' eyes. 
Further research is worthwhile to look into this relationship.</p><p id="p0165">Parallax
 affects the accuracy of eye fixation in stereoscopic tasks. The highest
 accuracy occurred when the participant's eyes fixated on the virtual 
ball projected at the screen, and accuracy was lowest when the virtual 
ball was projected 20 and 50â€¯cm in front of the screen. The accuracy of 
eye fixation improved when the deviation between the eye fixation 
location and the projected images of the virtual ball was low. The low 
eye fixation accuracy was caused by the high difficulty level of 
cognitive processing. The difficulty level of cognitive processing may 
be attributed to microsaccade, eye fixation movement tremor, and drift (<a name="bbib18" href="#bib18" class="workspace-trigger">Holmqvist et al., 2011</a>).
 Therefore, the accuracy declined when the virtual ball was projected at
 20 and 50â€¯cm in front of the screen. Another expected factor that 
influenced the accuracy of eye fixation was geometric distortions in the
 virtual environment. Distortion of the image such as minification, 
shear distortion, and pincushion distortion might occur in stereoscopic 
displays (<a name="bbib40" href="#bib40" class="workspace-trigger">Renner et al., 2013</a>).
 Since image distortion causes stretching of the image geometry away 
from the centre of the lens, projection of a virtual target closer to a 
participant may result in large distortion and inaccuracy of eye 
fixation.</p><p id="p0170">Eye fixation parameter results in 
stereoscopic displays have discrepancies with eye fixation parameter 
results in 2D or screen displays. <a name="bbib14" href="#bib14" class="workspace-trigger">Goldberg and Kotval (1999)</a>
 stated that participants performed fewer fixations with a good 
interface. In addition, a greater number of fixations were associated 
with longer completion time and longer time to first fixation, which 
were attributed to more confusion and difficulty in accomplishing a task
 in 2D or screen displays. In stereoscopic displays, the eye fixation 
parameter revealed that a smaller of fixations was associated with 
longer fixation duration, longer time to first fixation, and low 
accuracy. In stereoscopic displays, this condition arose when 
participants were required to perceive virtual balls close to their 
eyes. Therefore, participants will focus to perceive the virtual image 
in their fovea vision.</p><p id="p0175">Surprisingly, the results of eye
 fixation accuracy were above 0.85 in stereoscopic displays. The virtual
 ball closest to the participant's eyes yielded the lowest accuracy 
result. The findings indicated that eye fixation can be a promising 
technique for observing eye behaviour in stereoscopic displays. The 
highest accuracy of eye fixation indicated that significant performance 
can be achieved in stereoscopic displays. The findings of this study 
indicate that eye fixation may be a potential parameter to consider for 
designing effective tasks to improve usability of stereoscopic displays.</p></section><section id="sec5"><h2 id="sectitle0085">5. Conclusion</h2><p id="p0180">In
 this study, we experimentally investigated the eye fixation parameters 
in three different levels of parallax (at screen, and 20 and 50â€¯cm in 
front of the screen). Overall repeated measure ANOVA results showed that
 parallax had significant effects on eye fixation duration, number of 
fixations, time to first fixation, and accuracy. The important findings 
of this study were that smaller numbers of fixations were associated 
with longer fixation duration, longer times to first fixation, and low 
accuracy. The lowest number of fixations, longest fixation duration, 
longest time to first fixation, and lowest eye fixation accuracy 
occurred when the virtual ball was projected 50â€¯cm in front of the 
screen. That condition may be attributed to the vergence accommodation 
conflict, cognitive processing, and the ocular near triad when the 
virtual ball was projected closer to the participant's eyes. This paper 
presents important findings of parallax effects on the eye fixation 
parameter in stereoscopic displays, which can be considered an important
 parameter to improve the usability of stereoscopic displays. This study
 was limited to investigate the effects of parallax on the eye fixation 
parameter in stereoscopic displays. Further research will be required to
 address the visual fatigue symptoms and eye behaviour in real and 
stereoscopic displays.</p></section></div><section id="ack0010"><h2 id="sectitle0095">Acknowledgements</h2><p id="p0190">This work was supported by the <span id="gs1">Ministry of Science and Technology of Taiwan</span> (MOST <a href="#gs1">103-2221-E-011-100-MY3</a>).</p></section></div><div class="related-content-links u-hide-from-md"><button class="button button-anchor" type="button"><span class="button-text">Recommended articles</span></button><button class="button button-anchor" type="button"><span class="button-text">Citing articles (2)</span></button></div><div class="Tail"></div><section class="bibliography" id="cebib0010"><h2 class="section-title">References</h2><section class="bibliography-sec" id="cebibsec0010"><dl class="references" id="reference-links-cebibsec0010"><dt class="label"><a href="#bbib1" id="ref-id-bib1">Alshaer et al., 2017</a></dt><dd class="reference" id="sref1"><div class="contribution">A. Alshaer, H. Regenbrecht, D. Oâ€™hare<strong class="title">Immersion factors affecting perception and behaviour in a virtual reality power wheelchair simulator</strong></div><div class="host">Appl. Ergon., 58 (2017), pp. 1-12</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0003687016300904" aria-describedby="ref-id-sref1">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0003687016300904/pdfft?md5=5033a69d179e0432bf6267e7c90a8899&amp;pid=1-s2.0-S0003687016300904-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84969677264&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Immersion%20factors%20affecting%20perception%20and%20behaviour%20in%20a%20virtual%20reality%20power%20wheelchair%20simulator&amp;publication_year=2017&amp;author=A.%20Alshaer&amp;author=H.%20Regenbrecht&amp;author=D.%20O%E2%80%99hare">Google Scholar</a></div></dd><dt class="label"><a href="#bbib2" id="ref-id-bib2">Azuma et al., 2001</a></dt><dd class="reference" id="sref2"><div class="contribution">R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. Macintyre<strong class="title">Recent advances in augmented reality</strong></div><div class="host">IEEE Comput. Graph. Appl., 21 (2001), pp. 34-47</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/38.963459">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035503402&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Recent%20advances%20in%20augmented%20reality&amp;publication_year=2001&amp;author=R.%20Azuma&amp;author=Y.%20Baillot&amp;author=R.%20Behringer&amp;author=S.%20Feiner&amp;author=S.%20Julier&amp;author=B.%20Macintyre">Google Scholar</a></div></dd><dt class="label"><a href="#bbib3" id="ref-id-bib3">Bell and Fogler, 1995</a></dt><dd class="reference" id="sref3"><div class="contribution">J.T. Bell, H.S. Fogler<strong class="title">The investigation and application of virtual reality as an educational tool</strong></div><div class="host">Proceedings of the American Society for Engineering Education Annual Conference (1995), pp. 1718-1728</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029531967&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20investigation%20and%20application%20of%20virtual%20reality%20as%20an%20educational%20tool&amp;publication_year=1995&amp;author=J.T.%20Bell&amp;author=H.S.%20Fogler">Google Scholar</a></div></dd><dt class="label"><a href="#bbib4" id="ref-id-bib4">Benko et al., 2004</a></dt><dd class="reference" id="sref4"><div class="contribution">H. Benko, E.W. Ishak, S. Feiner<strong class="title">Collaborative mixed reality visualization of an archaeological excavation</strong></div><div class="host">Proceedings of the 3rd IEEE/ACM International Symposium on Mixed and Augmented Reality, IEEE Computer Society (2004), pp. 132-140</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/ISMAR.2004.23">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-18844445705&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Collaborative%20mixed%20reality%20visualization%20of%20an%20archaeological%20excavation&amp;publication_year=2004&amp;author=H.%20Benko&amp;author=E.W.%20Ishak&amp;author=S.%20Feiner">Google Scholar</a></div></dd><dt class="label"><a href="#bbib5" id="ref-id-bib5">Blignaut, 2009</a></dt><dd class="reference" id="sref5"><div class="contribution">P. Blignaut<strong class="title">Fixation identification: the optimum threshold for a dispersion algorithm</strong></div><div class="host">Atten. Percept. Psychophys., 71 (2009), pp. 881-895</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.3758/APP.71.4.881">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67650137115&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Fixation%20identification%3A%20the%20optimum%20threshold%20for%20a%20dispersion%20algorithm&amp;publication_year=2009&amp;author=P.%20Blignaut">Google Scholar</a></div></dd><dt class="label"><a href="#bbib6" id="ref-id-bib6">Burdea, 2002</a></dt><dd class="reference" id="sref6"><div class="contribution">G. Burdea<strong class="title">Keynote
 Address: Virtual Rehabilitation-benefits and Challenges. 1st 
International Workshop on Virtual Reality Rehabilitation (Mental Health,
 Neurological, Physical, Vocational) VRMHR</strong></div><div class="host"> (2002), pp. 1-11</div><div class="comment">sn</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-14044271074&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Keynote%20Address%3A%20Virtual%20Rehabilitation-benefits%20and%20Challenges.%201st%20International%20Workshop%20on%20Virtual%20Reality%20Rehabilitation%20%28Mental%20Health%2C%20Neurological%2C%20Physical%2C%20Vocational%29%20VRMHR&amp;publication_year=2002&amp;author=G.%20Burdea">Google Scholar</a></div></dd><dt class="label"><a href="#bbib7" id="ref-id-bib7">Council, 1983</a></dt><dd class="reference" id="sref7"><div class="contribution">N.R. Council<strong class="title">Video Displays, Work, and Vision</strong></div><div class="host">National Academies Press (1983)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Video%20Displays%2C%20Work%2C%20and%20Vision&amp;publication_year=1983&amp;author=N.R.%20Council">Google Scholar</a></div></dd><dt class="label"><a href="#bbib8" id="ref-id-bib8">Czernuszenko et al., 1997</a></dt><dd class="reference" id="sref8"><div class="contribution">M. Czernuszenko, D. Pape, D. Sandin, T. Defanti, G.L. Dawe, M.D. Brown<strong class="title">The ImmersaDesk and Infinity Wall projection-based virtual reality displays</strong></div><div class="host">ACM SIGGRAPH Comput. Graph., 31 (1997), pp. 46-49</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/271283.271303">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031140416&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20ImmersaDesk%20and%20Infinity%20Wall%20projection-based%20virtual%20reality%20displays&amp;publication_year=1997&amp;author=M.%20Czernuszenko&amp;author=D.%20Pape&amp;author=D.%20Sandin&amp;author=T.%20Defanti&amp;author=G.L.%20Dawe&amp;author=M.D.%20Brown">Google Scholar</a></div></dd><dt class="label"><a href="#bbib9" id="ref-id-bib9">Deutsch and Deutsch, 1963</a></dt><dd class="reference" id="sref9"><div class="contribution">J.A. Deutsch, D. Deutsch<strong class="title">Attention: some theoretical considerations</strong></div><div class="host">Psychol. Rev., 70 (1963), p. 80</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1037/h0039515">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-5144229363&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Attention%3A%20some%20theoretical%20considerations&amp;publication_year=1963&amp;author=J.A.%20Deutsch&amp;author=D.%20Deutsch">Google Scholar</a></div></dd><dt class="label"><a href="#bbib10" id="ref-id-bib10">Drewes, 2010</a></dt><dd class="reference" id="sref10"><div class="contribution">H. Drewes<strong class="title">Eye Gaze Tracking for Human Computer Interaction</strong></div><div class="host"> (2010)</div><div class="comment">(lmu)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20Gaze%20Tracking%20for%20Human%20Computer%20Interaction&amp;publication_year=2010&amp;author=H.%20Drewes">Google Scholar</a></div></dd><dt class="label"><a href="#bbib11" id="ref-id-bib11">Duchowski, 2007</a></dt><dd class="reference" id="sref11"><div class="contribution">A. Duchowski<strong class="title">Eye Tracking Methodology: Theory and Practice</strong></div><div class="host">Springer Science &amp; Business Media (2007)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20Tracking%20Methodology%3A%20Theory%20and%20Practice&amp;publication_year=2007&amp;author=A.%20Duchowski">Google Scholar</a></div></dd><dt class="label"><a href="#bbib12" id="ref-id-bib12">Emoto et al., 2005</a></dt><dd class="reference" id="sref12"><div class="contribution">M. Emoto, T. Niida, F. Okano<strong class="title">Repeated vergence adaptation causes the decline of visual functions in watching stereoscopic television</strong></div><div class="host">J. Disp. Technol., 1 (2005), pp. 328-340</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/JDT.2005.858938">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-29344460039&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Repeated%20vergence%20adaptation%20causes%20the%20decline%20of%20visual%20functions%20in%20watching%20stereoscopic%20television&amp;publication_year=2005&amp;author=M.%20Emoto&amp;author=T.%20Niida&amp;author=F.%20Okano">Google Scholar</a></div></dd><dt class="label"><a href="#bbib13" id="ref-id-bib13">Goldberg, 2014</a></dt><dd class="reference" id="sref13"><div class="contribution">J.H. Goldberg<strong class="title">Measuring software screen complexity: relating eye tracking, emotional valence, and subjective ratings</strong></div><div class="host">Int. J. Hum. Comput. Interact., 30 (2014), pp. 518-532</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/10447318.2014.906156">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84901262413&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Measuring%20software%20screen%20complexity%3A%20relating%20eye%20tracking%2C%20emotional%20valence%2C%20and%20subjective%20ratings&amp;publication_year=2014&amp;author=J.H.%20Goldberg">Google Scholar</a></div></dd><dt class="label"><a href="#bbib14" id="ref-id-bib14">Goldberg and Kotval, 1999</a></dt><dd class="reference" id="sref14"><div class="contribution">J.H. Goldberg, X.P. Kotval<strong class="title">Computer interface evaluation using eye movements: methods and constructs</strong></div><div class="host">Int. J. Ind. Ergon., 24 (1999), pp. 631-645</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0169814198000687" aria-describedby="ref-id-sref14">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0169814198000687/pdfft?md5=72ac7fd33c73b3629e401f15d9f79903&amp;pid=1-s2.0-S0169814198000687-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033215440&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Computer%20interface%20evaluation%20using%20eye%20movements%3A%20methods%20and%20constructs&amp;publication_year=1999&amp;author=J.H.%20Goldberg&amp;author=X.P.%20Kotval">Google Scholar</a></div></dd><dt class="label"><a href="#bbib15" id="ref-id-bib15">Hart and Karthigasu, 2007</a></dt><dd class="reference" id="sref15"><div class="contribution">R. Hart, K. Karthigasu<strong class="title">The benefits of virtual reality simulator training for laparoscopic surgery</strong></div><div class="host">Curr. Opin. Obstet. Gynecol., 19 (2007), pp. 297-302</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1097/GCO.0b013e328216f5b7">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34447581068&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20benefits%20of%20virtual%20reality%20simulator%20training%20for%20laparoscopic%20surgery&amp;publication_year=2007&amp;author=R.%20Hart&amp;author=K.%20Karthigasu">Google Scholar</a></div></dd><dt class="label"><a href="#bbib16" id="ref-id-bib16">Henderson, 1992</a></dt><dd class="reference" id="sref16"><div class="contribution">J.M. Henderson<strong class="title">Visual attention and eye movement control during reading and picture viewing</strong></div><div class="host">Eye Movements and Visual Cognition: Scene Perception and Reading, vol. 10 (1992), pp. 260-283</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/978-1-4612-2852-3_15">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20attention%20and%20eye%20movement%20control%20during%20reading%20and%20picture%20viewing&amp;publication_year=1992&amp;author=J.M.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bbib17" id="ref-id-bib17">Hoffman et al., 2008</a></dt><dd class="reference" id="sref17"><div class="contribution">D.M. Hoffman, A.R. Girshick, K. Akeley, M.S. Banks<strong class="title">Vergenceâ€“accommodation conflicts hinder visual performance and cause visual fatigue</strong></div><div class="host">J. Vis., 8 (2008)</div><div class="comment">33â€“33</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Vergence%E2%80%93accommodation%20conflicts%20hinder%20visual%20performance%20and%20cause%20visual%20fatigue&amp;publication_year=2008&amp;author=D.M.%20Hoffman&amp;author=A.R.%20Girshick&amp;author=K.%20Akeley&amp;author=M.S.%20Banks">Google Scholar</a></div></dd><dt class="label"><a href="#bbib18" id="ref-id-bib18">Holmqvist et al., 2011</a></dt><dd class="reference" id="sref18"><div class="contribution">K. Holmqvist, M.M. Nystr, R. Andersson, R. Dewhurst, H. Jarodzka, J. Van de weijer<strong class="title">Eye Tracking: A Comprehensive Guide to Methods and Measures</strong></div><div class="host">OUP Oxford (2011)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20Tracking%3A%20A%20Comprehensive%20Guide%20to%20Methods%20and%20Measures&amp;publication_year=2011&amp;author=K.%20Holmqvist&amp;author=M.M.%20Nystr&amp;author=R.%20Andersson&amp;author=R.%20Dewhurst&amp;author=H.%20Jarodzka&amp;author=J.%20Van%20de%20weijer">Google Scholar</a></div></dd><dt class="label"><a href="#bbib19" id="ref-id-bib19">Knight and Baber, 2007</a></dt><dd class="reference" id="sref19"><div class="contribution">J.F. Knight, C. Baber<strong class="title">Effect of head-mounted displays on posture</strong></div><div class="host">Hum. Factors, 49 (2007), pp. 797-807</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1518/001872007X230172">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34548848520&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Effect%20of%20head-mounted%20displays%20on%20posture&amp;publication_year=2007&amp;author=J.F.%20Knight&amp;author=C.%20Baber">Google Scholar</a></div></dd><dt class="label"><a href="#bbib20" id="ref-id-bib20">Kolers et al., 1981</a></dt><dd class="reference" id="sref20"><div class="contribution">P.A. Kolers, R.L. Duchnicky, D.C. Ferguson<strong class="title">Eye movement measurement of readability of CRT displays</strong></div><div class="host">Hum. Factors. J. Hum. Factors Ergon. Soc., 23 (1981), pp. 517-527</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1177/001872088102300502">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0019632274&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movement%20measurement%20of%20readability%20of%20CRT%20displays&amp;publication_year=1981&amp;author=P.A.%20Kolers&amp;author=R.L.%20Duchnicky&amp;author=D.C.%20Ferguson">Google Scholar</a></div></dd><dt class="label"><a href="#bbib21" id="ref-id-bib21">Lambooij et al., 2009</a></dt><dd class="reference" id="sref21"><div class="contribution">M. Lambooij, M. Fortuin, I. Heynderickx, W. Ijsselsteijn<strong class="title">Visual discomfort and visual fatigue of stereoscopic displays: a review</strong></div><div class="host">J. Imag. Sci. Technol., 53 (2009)</div><div class="comment">30201â€“1-30201-14</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20discomfort%20and%20visual%20fatigue%20of%20stereoscopic%20displays%3A%20a%20review&amp;publication_year=2009&amp;author=M.%20Lambooij&amp;author=M.%20Fortuin&amp;author=I.%20Heynderickx&amp;author=W.%20Ijsselsteijn">Google Scholar</a></div></dd><dt class="label"><a href="#bbib22" id="ref-id-bib22">Lang et al., 2010</a></dt><dd class="reference" id="sref22"><div class="contribution">M. Lang, A. Hornung, O. Wang, S. Poulakos, A. Smolic, M. Gross<strong class="title">Nonlinear disparity mapping for stereoscopic 3D</strong></div><div class="host">ACM Trans. Graph., 29 (2010), p. 75</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1002/9783433600368.ch4">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77952640802&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Nonlinear%20disparity%20mapping%20for%20stereoscopic%203D&amp;publication_year=2010&amp;author=M.%20Lang&amp;author=A.%20Hornung&amp;author=O.%20Wang&amp;author=S.%20Poulakos&amp;author=A.%20Smolic&amp;author=M.%20Gross">Google Scholar</a></div></dd><dt class="label"><a href="#bbib23" id="ref-id-bib23">Lawson et al., 2016</a></dt><dd class="reference" id="sref23"><div class="contribution">G. Lawson, D. Salanitri, B. Waterfield<strong class="title">Future directions for the development of virtual reality within an automotive manufacturer</strong></div><div class="host">Appl. Ergon., 53 (2016), pp. 323-330</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0003687015300260" aria-describedby="ref-id-sref23">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0003687015300260/pdfft?md5=707578bb31b77c6706a10e96a54f784b&amp;pid=1-s2.0-S0003687015300260-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84958648699&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Future%20directions%20for%20the%20development%20of%20virtual%20reality%20within%20an%20automotive%20manufacturer&amp;publication_year=2016&amp;author=G.%20Lawson&amp;author=D.%20Salanitri&amp;author=B.%20Waterfield">Google Scholar</a></div></dd><dt class="label"><a href="#bbib24" id="ref-id-bib24">Lebreton et al., 2012</a></dt><dd class="reference" id="sref24"><div class="contribution">P. Lebreton, A. Raake, M. Barkowsky, P. Le callet<strong class="title">Evaluating depth perception of 3D stereoscopic videos</strong></div><div class="host">IEEE J. Sel. Top. Signal Process., 6 (2012), pp. 710-720</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/JSTSP.2012.2213236">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866495334&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Evaluating%20depth%20perception%20of%203D%20stereoscopic%20videos&amp;publication_year=2012&amp;author=P.%20Lebreton&amp;author=A.%20Raake&amp;author=M.%20Barkowsky&amp;author=P.%20Le%20callet">Google Scholar</a></div></dd><dt class="label"><a href="#bbib25" id="ref-id-bib25">Lin and Widyaningrum, 2016</a></dt><dd class="reference" id="sref25"><div class="contribution">C.J. Lin, R. Widyaningrum<strong class="title">Eye pointing in stereoscopic displays</strong></div><div class="host">J. Eye Mov. Res., 9 (2016)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20pointing%20in%20stereoscopic%20displays&amp;publication_year=2016&amp;author=C.J.%20Lin&amp;author=R.%20Widyaningrum">Google Scholar</a></div></dd><dt class="label"><a href="#bbib26" id="ref-id-bib26">Lin and Woldegiorgis, 2017</a></dt><dd class="reference" id="sref26"><div class="contribution">C.J. Lin, B.H. Woldegiorgis<strong class="title">Egocentric distance perception and performance of direct pointing in stereoscopic displays</strong></div><div class="host">Appl. Ergon., 64 (2017), pp. 66-74</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0003687017301187" aria-describedby="ref-id-sref26">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0003687017301187/pdfft?md5=3f091fcd9b320489028d4f248a331425&amp;pid=1-s2.0-S0003687017301187-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85019832304&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Egocentric%20distance%20perception%20and%20performance%20of%20direct%20pointing%20in%20stereoscopic%20displays&amp;publication_year=2017&amp;author=C.J.%20Lin&amp;author=B.H.%20Woldegiorgis">Google Scholar</a></div></dd><dt class="label"><a href="#bbib27" id="ref-id-bib27">Lin et al., 2015a</a></dt><dd class="reference" id="sref27"><div class="contribution">C.J. Lin, H.J. Chen, P.Y. Cheng, T.L. Sun<strong class="title">Effects of displays on visually controlled task performance in three-dimensional virtual reality environment</strong></div><div class="host">Hum. Factors .Ergon. Manuf. Serv. Ind., 25 (2015), pp. 523-533</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1002/hfm.20566">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84970937415&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Effects%20of%20displays%20on%20visually%20controlled%20task%20performance%20in%20three-dimensional%20virtual%20reality%20environment&amp;publication_year=2015&amp;author=C.J.%20Lin&amp;author=H.J.%20Chen&amp;author=P.Y.%20Cheng&amp;author=T.L.%20Sun">Google Scholar</a></div></dd><dt class="label"><a href="#bbib28" id="ref-id-bib28">Lin et al., 2015b</a></dt><dd class="reference" id="sref28"><div class="contribution">C.J. Lin, B.H. Woldegiorgis, D. Caesaron, L.-Y. Cheng<strong class="title">Distance estimation with mixed real and virtual targets in stereoscopic displays</strong></div><div class="host">Displays, 36 (2015), pp. 41-48</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0141938214000912" aria-describedby="ref-id-sref28">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0141938214000912/pdfft?md5=9699481fba48352e38bfdd9f30a43774&amp;pid=1-s2.0-S0141938214000912-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.3901/JME.2015.16.041">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85027929726&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Distance%20estimation%20with%20mixed%20real%20and%20virtual%20targets%20in%20stereoscopic%20displays&amp;publication_year=2015&amp;author=C.J.%20Lin&amp;author=B.H.%20Woldegiorgis&amp;author=D.%20Caesaron&amp;author=L.-Y.%20Cheng">Google Scholar</a></div></dd><dt class="label"><a href="#bbib29" id="ref-id-bib29">Mackenzie, 1843</a></dt><dd class="reference" id="sref29"><div class="contribution">W. Mackenzie<strong class="title">Asthenopia</strong></div><div class="host">Ann. Oncol., 10 (1843), pp. 97-115</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Asthenopia&amp;publication_year=1843&amp;author=W.%20Mackenzie">Google Scholar</a></div></dd><dt class="label"><a href="#bbib30" id="ref-id-bib30">Manor and Gordon, 2003</a></dt><dd class="reference" id="sref30"><div class="contribution">B.R. Manor, E. Gordon<strong class="title">Defining the temporal threshold for ocular fixation in free-viewing visuocognitive tasks</strong></div><div class="host">J. Neurosci. Meth., 128 (2003), pp. 85-93</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0165027003001511" aria-describedby="ref-id-sref30">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0165027003001511/pdfft?md5=b4dd99b477a3ebe774832b454545c693&amp;pid=1-s2.0-S0165027003001511-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0042389589&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Defining%20the%20temporal%20threshold%20for%20ocular%20fixation%20in%20free-viewing%20visuocognitive%20tasks&amp;publication_year=2003&amp;author=B.R.%20Manor&amp;author=E.%20Gordon">Google Scholar</a></div></dd><dt class="label"><a href="#bbib31" id="ref-id-bib31">Mcneill et al., 2004</a></dt><dd class="reference" id="sref31"><div class="contribution">M. Mcneill, L. Pokluda, S. Mcdonough, J. Crosbie<strong class="title">Immersive virtual reality for upper limb rehabilitation following stroke. Systems, Man and Cybernetics</strong></div><div class="host">IEEE International Conference on, 2004. IEEE (2004), pp. 2783-2789</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/ICSMC.2004.1400754">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-15744393732&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Immersive%20virtual%20reality%20for%20upper%20limb%20rehabilitation%20following%20stroke.%20Systems%2C%20Man%20and%20Cybernetics&amp;publication_year=2004&amp;author=M.%20Mcneill&amp;author=L.%20Pokluda&amp;author=S.%20Mcdonough&amp;author=J.%20Crosbie">Google Scholar</a></div></dd><dt class="label"><a href="#bbib32" id="ref-id-bib32">Mon-williams et al., 1995</a></dt><dd class="reference" id="sref32"><div class="contribution">M. Mon-williams, J. Wann, S. Rushton<strong class="title">Design factors in stereoscopic virtual-reality displays</strong></div><div class="host">J. Soc. Inf. Disp., 3 (1995), pp. 207-210</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1889/1.1984970">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029521517&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Design%20factors%20in%20stereoscopic%20virtual-reality%20displays&amp;publication_year=1995&amp;author=M.%20Mon-williams&amp;author=J.%20Wann&amp;author=S.%20Rushton">Google Scholar</a></div></dd><dt class="label"><a href="#bbib33" id="ref-id-bib33">Mujber et al., 2004</a></dt><dd class="reference" id="sref33"><div class="contribution">T.S. Mujber, T. Szecsi, M.S. Hashmi<strong class="title">Virtual reality applications in manufacturing process simulation</strong></div><div class="host">J. Mater. Process. Technol., 155 (2004), pp. 1834-1838</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S0924013604005618" aria-describedby="ref-id-sref33">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S0924013604005618/pdfft?md5=f933f53a4e395d208d3a3d5e0e6c6a9a&amp;pid=1-s2.0-S0924013604005618-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-10044285235&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Virtual%20reality%20applications%20in%20manufacturing%20process%20simulation&amp;publication_year=2004&amp;author=T.S.%20Mujber&amp;author=T.%20Szecsi&amp;author=M.S.%20Hashmi">Google Scholar</a></div></dd><dt class="label"><a href="#bbib34" id="ref-id-bib34">Okada et al., 2006</a></dt><dd class="reference" id="sref34"><div class="contribution">Y. Okada, K. Ukai, J.S. Wolffsohn, B. Gilmartin, A. Iijima, T. Bando<strong class="title">Target spatial frequency determines the response to conflicting defocus-and convergence-driven accommodative stimuli</strong></div><div class="host">Vis. Res., 46 (2006), pp. 475-484</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S004269890500355X" aria-describedby="ref-id-sref34">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S004269890500355X/pdfft?md5=46d5dd91ece7d3a1994d363dc379686a&amp;pid=1-s2.0-S004269890500355X-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-29144498211&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Target%20spatial%20frequency%20determines%20the%20response%20to%20conflicting%20defocus-and%20convergence-driven%20accommodative%20stimuli&amp;publication_year=2006&amp;author=Y.%20Okada&amp;author=K.%20Ukai&amp;author=J.S.%20Wolffsohn&amp;author=B.%20Gilmartin&amp;author=A.%20Iijima&amp;author=T.%20Bando">Google Scholar</a></div></dd><dt class="label"><a href="#bbib35" id="ref-id-bib35">Ooms et al., 2015</a></dt><dd class="reference" id="sref35"><div class="contribution">K. Ooms, L. Dupont, L. Lapon, S. Popelka<strong class="title">Accuracy and precision of fixation locations recorded with the low-cost Eye Tribe tracker in different experimental set-ups</strong></div><div class="host">J. Eye Mov. Res., 8 (2015)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Accuracy%20and%20precision%20of%20fixation%20locations%20recorded%20with%20the%20low-cost%20Eye%20Tribe%20tracker%20in%20different%20experimental%20set-ups&amp;publication_year=2015&amp;author=K.%20Ooms&amp;author=L.%20Dupont&amp;author=L.%20Lapon&amp;author=S.%20Popelka">Google Scholar</a></div></dd><dt class="label"><a href="#bbib36" id="ref-id-bib36">Pan et al., 2004</a></dt><dd class="reference" id="sref36"><div class="contribution">B. Pan, H.A. Hembrooke, G.K. Gay, L.A. Granka, M.K. Feusner, J.K. Newman<strong class="title">The determinants of web page viewing behavior: an eye-tracking study</strong></div><div class="host">Proceedings of the 2004 Symposium on Eye Tracking Research &amp; Applications, ACM (2004), pp. 147-154</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/968363.968391">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-2442509988&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20determinants%20of%20web%20page%20viewing%20behavior%3A%20an%20eye-tracking%20study&amp;publication_year=2004&amp;author=B.%20Pan&amp;author=H.A.%20Hembrooke&amp;author=G.K.%20Gay&amp;author=L.A.%20Granka&amp;author=M.K.%20Feusner&amp;author=J.K.%20Newman">Google Scholar</a></div></dd><dt class="label"><a href="#bbib37" id="ref-id-bib37">Parsons and Mitchell, 2002</a></dt><dd class="reference" id="sref37"><div class="contribution">S. Parsons, P. Mitchell<strong class="title">The potential of virtual reality in social skills training for people with autistic spectrum disorders</strong></div><div class="host">J. Intellect. Disabil. Res., 46 (2002), pp. 430-443</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1046/j.1365-2788.2002.00425.x">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036316792&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20potential%20of%20virtual%20reality%20in%20social%20skills%20training%20for%20people%20with%20autistic%20spectrum%20disorders&amp;publication_year=2002&amp;author=S.%20Parsons&amp;author=P.%20Mitchell">Google Scholar</a></div></dd><dt class="label"><a href="#bbib38" id="ref-id-bib38">Petkov, 2012</a></dt><dd class="reference" id="sref38"><div class="contribution">E. Petkov<strong class="title">Generation of stereo images in 3D graphics applications for stereoscopic and nonstereoscopic displays</strong></div><div class="host">Int. J. Adv. Sci. Technol., 4 (2012)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Generation%20of%20stereo%20images%20in%203D%20graphics%20applications%20for%20stereoscopic%20and%20nonstereoscopic%20displays&amp;publication_year=2012&amp;author=E.%20Petkov">Google Scholar</a></div></dd><dt class="label"><a href="#bbib39" id="ref-id-bib39">Reichle et al., 2003</a></dt><dd class="reference" id="sref39"><div class="contribution">E.D. Reichle, K. Rayner, A. Pollatsek<strong class="title">The EZ Reader model of eye-movement control in reading: comparisons to other models</strong></div><div class="host">Behav. Brain Sci., 26 (2003), pp. 445-476</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0038569943&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20EZ%20Reader%20model%20of%20eye-movement%20control%20in%20reading%3A%20comparisons%20to%20other%20models&amp;publication_year=2003&amp;author=E.D.%20Reichle&amp;author=K.%20Rayner&amp;author=A.%20Pollatsek">Google Scholar</a></div></dd><dt class="label"><a href="#bbib40" id="ref-id-bib40">Renner et al., 2013</a></dt><dd class="reference" id="sref40"><div class="contribution">R.S. Renner, B.M. Velichkovsky, J.R. Helmert<strong class="title">The perception of egocentric distances in virtual environments - a review</strong></div><div class="host">ACM Comput. Surv., 46 (2013), pp. 1-32</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84876395781&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20perception%20of%20egocentric%20distances%20in%20virtual%20environments%20-%20a%20review&amp;publication_year=2013&amp;author=R.S.%20Renner&amp;author=B.M.%20Velichkovsky&amp;author=J.R.%20Helmert">Google Scholar</a></div></dd><dt class="label"><a href="#bbib41" id="ref-id-bib41">Salvucci and Goldberg, 2000</a></dt><dd class="reference" id="sref41"><div class="contribution">D.D. Salvucci, J.H. Goldberg<strong class="title">Identifying fixations and saccades in eye-tracking protocols</strong></div><div class="host">Proceedings of the 2000 Symposium on Eye Tracking Research &amp; Applications, ACM (2000), pp. 71-78</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/355017.355028">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034592439&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Identifying%20fixations%20and%20saccades%20in%20eye-tracking%20protocols&amp;publication_year=2000&amp;author=D.D.%20Salvucci&amp;author=J.H.%20Goldberg">Google Scholar</a></div></dd><dt class="label"><a href="#bbib52" id="ref-id-bib52">Seigle, 2009</a></dt><dd class="reference" id="sref52"><div class="contribution">D. Seigle<strong class="title">3rd dimension: dimensionalization</strong></div><div class="host">Veritas Visus, 4 (3) (2009), pp. 69-75</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85026434401&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=3rd%20dimension%3A%20dimensionalization&amp;publication_year=2009&amp;author=D.%20Seigle">Google Scholar</a></div></dd><dt class="label"><a href="#bbib42" id="ref-id-bib42">Sharples et al., 2008</a></dt><dd class="reference" id="sref42"><div class="contribution">S. Sharples, S. Cobb, A. Moody, J.R. Wilson<strong class="title">Virtual
 reality induced symptoms and effects (VRISE): comparison of head 
mounted display (HMD), desktop and projection display systems</strong></div><div class="host">Displays, 29 (2008), pp. 58-69</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/S014193820700100X" aria-describedby="ref-id-sref42">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/S014193820700100X/pdfft?md5=aae3ae4537295f858f01990525fbc0e7&amp;pid=1-s2.0-S014193820700100X-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-37249078299&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Virtual%20reality%20induced%20symptoms%20and%20effects%20%28VRISE%29%3A%20comparison%20of%20head%20mounted%20display%20%28HMD%29%2C%20desktop%20and%20projection%20display%20systems&amp;publication_year=2008&amp;author=S.%20Sharples&amp;author=S.%20Cobb&amp;author=A.%20Moody&amp;author=J.R.%20Wilson">Google Scholar</a></div></dd><dt class="label"><a href="#bbib43" id="ref-id-bib43">Shic et al., 2008</a></dt><dd class="reference" id="sref43"><div class="contribution">F. Shic, B. Scassellati, K. Chawarska<strong class="title">The incomplete fixation measure</strong></div><div class="host">Proceedings of the 2008 Symposium on Eye Tracking Research &amp; Applications, ACM (2008), pp. 111-114</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/1344471.1344500">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77950323764&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20incomplete%20fixation%20measure&amp;publication_year=2008&amp;author=F.%20Shic&amp;author=B.%20Scassellati&amp;author=K.%20Chawarska">Google Scholar</a></div></dd><dt class="label"><a href="#bbib44" id="ref-id-bib44">Smith et al., 2012</a></dt><dd class="reference" id="sref44"><div class="contribution">R. Smith, A. Day, T. Rockall, K. Ballard, M. Bailey, I. Jourdan<strong class="title">Advanced stereoscopic projection technology significantly improves novice performance of minimally invasive surgical skills</strong></div><div class="host">Surg. Endosc., 26 (2012), pp. 1522-1527</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/s00464-011-2080-8">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84863988604&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Advanced%20stereoscopic%20projection%20technology%20significantly%20improves%20novice%20performance%20of%20minimally%20invasive%20surgical%20skills&amp;publication_year=2012&amp;author=R.%20Smith&amp;author=A.%20Day&amp;author=T.%20Rockall&amp;author=K.%20Ballard&amp;author=M.%20Bailey&amp;author=I.%20Jourdan">Google Scholar</a></div></dd><dt class="label"><a href="#bbib45" id="ref-id-bib45">Thornton et al., 2005</a></dt><dd class="reference" id="sref45"><div class="contribution">M. Thornton, S. Marshall, J. Mccomas, H. Finestone, A. Mccormick, H. Sveistrup<strong class="title">Benefits
 of activity and virtual reality based balance exercise programmes for 
adults with traumatic brain injury: perceptions of participants and 
their caregivers</strong></div><div class="host">Brain Inj., 19 (2005), pp. 989-1000</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/02699050500109944">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Benefits%20of%20activity%20and%20virtual%20reality%20based%20balance%20exercise%20programmes%20for%20adults%20with%20traumatic%20brain%20injury%3A%20perceptions%20of%20participants%20and%20their%20caregivers&amp;publication_year=2005&amp;author=M.%20Thornton&amp;author=S.%20Marshall&amp;author=J.%20Mccomas&amp;author=H.%20Finestone&amp;author=A.%20Mccormick&amp;author=H.%20Sveistrup">Google Scholar</a></div></dd><dt class="label"><a href="#bbib46" id="ref-id-bib46">Valkov et al., 2011</a></dt><dd class="reference" id="sref46"><div class="contribution">D. Valkov, F. Steinicke, G. Bruder, K. Hinrichs<strong class="title">2d touching of 3d stereoscopic objects</strong></div><div class="host">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM (2011), pp. 1353-1362</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/1978942.1979142">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79958102239&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=2d%20touching%20of%203d%20stereoscopic%20objects&amp;publication_year=2011&amp;author=D.%20Valkov&amp;author=F.%20Steinicke&amp;author=G.%20Bruder&amp;author=K.%20Hinrichs">Google Scholar</a></div></dd><dt class="label"><a href="#bbib47" id="ref-id-bib47">Von Helmholz, 1897</a></dt><dd class="reference" id="sref47"><div class="contribution">H. Von Helmholz<strong class="title">Handbuch Der Physiologischen Optik (Handbook of Physiological Optics)</strong></div><div class="host">JSTOR (1897)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Handbuch%20Der%20Physiologischen%20Optik%20%28Handbook%20of%20Physiological%20Optics%29&amp;publication_year=1897&amp;author=H.%20Von%20Helmholz">Google Scholar</a></div></dd><dt class="label"><a href="#bbib48" id="ref-id-bib48">Von noorden and Campos, 2002</a></dt><dd class="reference" id="sref48"><div class="contribution">G. Von noorden, E. Campos<strong class="title">Physiology of the Ocular Movements. Binocular Vision and Ocular Motility: Theory and Management of Strabismus</strong></div><div class="host"> (sixth ed.), Mosby, St Louis (2002), pp. 52-84</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Physiology%20of%20the%20Ocular%20Movements.%20Binocular%20Vision%20and%20Ocular%20Motility%3A%20Theory%20and%20Management%20of%20Strabismus&amp;publication_year=2002&amp;author=G.%20Von%20noorden&amp;author=E.%20Campos">Google Scholar</a></div></dd><dt class="label"><a href="#bbib49" id="ref-id-bib49">Wang et al., 2012</a></dt><dd class="reference" id="sref49"><div class="contribution">J. Wang, P. Le callet, S. Tourancheau, V. Ricordel, M.P. Da silva<strong class="title">Study of depth bias of observers in free viewing of still stereoscopic synthetic stimuli</strong></div><div class="host">J. Eye Mov. Res., 5 (2012)</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Study%20of%20depth%20bias%20of%20observers%20in%20free%20viewing%20of%20still%20stereoscopic%20synthetic%20stimuli&amp;publication_year=2012&amp;author=J.%20Wang&amp;author=P.%20Le%20callet&amp;author=S.%20Tourancheau&amp;author=V.%20Ricordel&amp;author=M.P.%20Da%20silva">Google Scholar</a></div></dd><dt class="label"><a href="#bbib50" id="ref-id-bib50">Wann et al., 1995</a></dt><dd class="reference" id="sref50"><div class="contribution">J.P. Wann, S. Rushton, M. Mon-williams<strong class="title">Natural problems for stereoscopic depth perception in virtual environments</strong></div><div class="host">Vis. Res., 35 (1995), pp. 2731-2736</div><div class="ReferenceLinks"><a class="link" href="https://www.sciencedirect.com/science/article/pii/004269899500018U" aria-describedby="ref-id-sref50">Article</a><a class="anchor pdf link" href="https://www.sciencedirect.com/science/article/pii/004269899500018U/pdf?md5=f5cc53f906f5149cdf5439afcf691fbe&amp;pid=1-s2.0-004269899500018U-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029123819&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Natural%20problems%20for%20stereoscopic%20depth%20perception%20in%20virtual%20environments&amp;publication_year=1995&amp;author=J.P.%20Wann&amp;author=S.%20Rushton&amp;author=M.%20Mon-williams">Google Scholar</a></div></dd><dt class="label"><a href="#bbib51" id="ref-id-bib51">Zhou et al., 2008</a></dt><dd class="reference" id="sref51"><div class="contribution">F. Zhou, H.B.-L. Duh, M. Billinghurst<strong class="title">Trends in augmented reality tracking, interaction and display: a review of ten years of ISMAR</strong></div><div class="host">Proceedings of the 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, IEEE Computer Society (2008), pp. 193-202</div><div class="ReferenceLinks"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-56349160133&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Trends%20in%20augmented%20reality%20tracking%2C%20interaction%20and%20display%3A%20a%20review%20of%20ten%20years%20of%20ISMAR&amp;publication_year=2008&amp;author=F.%20Zhou&amp;author=H.B.-L.%20Duh&amp;author=M.%20Billinghurst">Google Scholar</a></div></dd></dl></section></section><div class="Copyright"><span class="copyright-line">Â© 2018 Elsevier Ltd. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class="RelatedContent" aria-label="Related content"><section class="SidePanel"><header id="recommended-articles-header" class="side-panel-header"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" type="button"><span class="button-link-text"><h2 class="section-title">Recommended articles</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0003687018300036"><h3 class="article-title ellipsis" id="recommended-articles-article0-title" title="Influence of folding mechanism of bicycles on their usability"><span>Influence of folding mechanism of bicycles on their usability</span></h3></a><div class="article-source ellipsis"><div class="source">Applied Ergonomics, Volume 69, 2018, pp. 58-65</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0003687018300036/pdfft?md5=790432733d17827976ac0816a61bb576&amp;pid=1-s2.0-S0003687018300036-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle" aria-describedby="recommended-articles-article0-title" aria-controls="recommended-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0003687017301187"><h3 class="article-title ellipsis" id="recommended-articles-article1-title" title="Egocentric distance perception and performance of direct pointing in stereoscopic displays"><span>Egocentric distance perception and performance of direct pointing in stereoscopic displays</span></h3></a><div class="article-source ellipsis"><div class="source">Applied Ergonomics, Volume 64, 2017, pp. 66-74</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0003687017301187/pdfft?md5=3f091fcd9b320489028d4f248a331425&amp;pid=1-s2.0-S0003687017301187-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle" aria-describedby="recommended-articles-article1-title" aria-controls="recommended-articles-article1" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article1" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S000368701730282X"><h3 class="article-title ellipsis" id="recommended-articles-article2-title" title="Virtual reality sickness questionnaire (VRSQ): Motion sickness measurement index in a virtual reality environment"><span>Virtual reality sickness questionnaire (VRSQ): Motion sickness measurement index in a virtual reality environment</span></h3></a><div class="article-source ellipsis"><div class="source">Applied Ergonomics, Volume 69, 2018, pp. 66-73</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S000368701730282X/pdfft?md5=44b92e2bc52f2386609bc3a18efed20c&amp;pid=1-s2.0-S000368701730282X-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle" aria-describedby="recommended-articles-article2-title" aria-controls="recommended-articles-article2" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article2" aria-hidden="true"></div></li></ul></div><div class="pagination u-position-relative u-padding-xs-bottom"><span class="u-position-absolute"></span><span class="pagination-pages-label"><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class="u-position-absolute"><button class="button-link button-link-secondary next-button" type="button"><span class="button-link-text">Next</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel"><header id="citing-articles-header" class="side-panel-header"><button class="button-link side-panel-toggle button-link-primary" aria-expanded="false" type="button"><span class="button-link-text"><h2 class="section-title">Citing articles (2)</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="u-display-none" aria-hidden="true" aria-describedby="citing-articles-header"><div id="citing-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S020852161830250X" target="_self"><h3 class="article-title ellipsis" id="citing-articles-article0-title" title="Eye and EEG activity markers for visual comfort level of images">Eye and EEG activity markers for visual comfort level of images</h3></a><div class="article-source ellipsis">2018, Biocybernetics and Biomedical Engineering</div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S020852161830250X/pdfft?md5=769c8f121fad112a60bcd6b166f2dc88&amp;pid=1-s2.0-S020852161830250X-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle" aria-describedby="citing-articles-article0-title" aria-controls="citing-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://doi.org/10.16910/jemr.11.6.3" target="_blank"><h3 class="article-title ellipsis" id="citing-articles-article1-title" title="Eye movement parameters for performance evaluation in projection-based stereoscopic display">Eye movement parameters for performance evaluation in projection-based stereoscopic display</h3></a><div class="article-source ellipsis">2018, Journal of Eye Movement Research</div></div><div class="buttons"><button class="button-link button-link-secondary side-panel-details-toggle" aria-describedby="citing-articles-article1-title" aria-controls="citing-articles-article1" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article1" aria-hidden="true"></div></li></ul></div></div></section><section class="SidePanel hidden"><header id="metrics-header" class="side-panel-header"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" type="button"><span class="button-link-text"><h2 class="section-title">Article Metrics</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><a href="https://plu.mx/plum/a/?doi=10.1016/j.apergo.2017.12.020" class="plumx-summary plum-sciencedirect-theme" data-pass-hidden-categories="true" data-hide-usage="true" data-orientation="vertical" data-hide-print="true" data-site="plum" data-on-success="onMetricsWidgetSuccess">View article metrics</a></div></section></aside></div></div><div></div></div><div id="footer"><div class="hor-line" style="border-color:#e9711c"></div><div class="panel-s u-padding-l-bottom u-bg-white u-clr-grey7" role="contentinfo"><a class="anchor move-left els-footer-elsevier anchor-has-inherit-color" href="https://www.elsevier.com/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text"><svg viewBox="-3345 3440.027 140.01 24.333" style="width:104px;height:30px"><title>Elsevier</title><path id="E" style="fill:#E9711C" d="M-3343.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3344L-3343.999,3461.698"></path><path style="fill:#E9711C" d="M-3325.448,3461.698c2.074-0.118,2.83-0.502,2.83-2.832v-13.511c0-2.33-0.757-2.715-2.83-2.832v-0.591h8.884 v0.591c-2.243,0-3.027,0.472-3.027,2.891v13.009c0,1.652,0.056,2.625,1.71,2.625h4.008c3,0,4.4-2,5.576-4.1l0.673,0.118 l-1.71,5.222h-16.114V3461.698"></path><path style="fill:#E9711C" d="M-3307.122,3456.27h0.561c1.12,2.596,2.886,5.4,5.94,5.4c2,0,3.672-1.3,3.672-3.334 c0-1.652-1.626-3.1-4.176-4.927c-3.28-2.36-5.41-3.746-5.41-6.43c0-3.422,2.55-5.517,5.633-5.517c2.214,0,3,0.944,4.204,0.944 c0.476,0,0.56-0.266,0.476-0.737h0.561l0.645,6.076h-0.561c-0.785-2.625-2.523-5.19-5.354-5.19c-1.737,0-3.138,1.356-3.138,3.185 c0,1.918,1.933,3.157,5.016,5.016c2.523,1.504,5,3.362,5,6.254c0,3.245-2.608,5.752-5.97,5.752c-2.02,0-4.148-0.855-4.68-0.855 c-0.336,0-0.7,0.207-0.756,0.649h-0.56l-1.094-6.282"></path><path style="fill:#E9711C" d="M-3293.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3294L-3293.999,3461.698"></path><path style="fill:#E9711C" d="M-3265.839,3462.524h-0.42l-5.41-12.538c-0.896-2.065-1.71-4.16-2.83-6.136 c-0.478-0.826-1.346-1.327-2.3-1.327v-0.591h8.323v0.591c-0.785,0-2.354,0-2.354,1.15c0,0.384,0.87,2.45,1.653,4.308l4.063,9.676 l4.877-11.359c0.588-1.356,0.757-2.094,0.757-2.713c0-0.618-0.673-0.974-2.13-1.062v-0.59h5.941v0.591 c-0.337,0.06-0.7,0.117-1.037,0.295c-1.066,0.56-2.13,3.57-2.635,4.749l-6.5,14.957"></path><path style="fill:#E9711C" d="M-3255.472,3461.698c2.24,0,3.025-0.473,3.025-2.892v-13.393c0-2.42-0.784-2.89-3.025-2.891v-0.59h9.078v0.591 c-2.24,0-3.025,0.472-3.025,2.891v13.393c0,2.42,0.784,2.892,3.025,2.892v0.59h-9.08v-0.59"></path><path id="E_2_" style="fill:#E9711C" d="M-3244.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3245L-3244.999,3461.698"></path><path style="fill:#E9711C" d="M-3206,3461.698c-1.26-0.354-1.71-0.68-2.466-1.623l-6.166-7.609c3.027-0.65,5.13-2.185,5.13-5.547 c0-4.75-4.26-4.986-7.764-4.986h-9.191v0.591c2.24,0,3.026,0.472,3.026,2.891v13.393c0,2.42-0.785,2.892-3.026,2.892v0.59h9.08 v-0.59c-2.242,0-3.027-0.473-3.027-2.892v-5.604h2.551l7.314,9.086h4.54L-3206,3461.698 M-3220.399,3444.499 c0-1.387,0.337-1.476,2.186-1.476c2.774,0,5.3,0.974,5.3,4.308c0,3.6-2.887,4.63-5.914,4.631h-1.569v-7.463H-3220.399z"></path></svg></span></a><div class="panel-s u-bg-white u-padding-0-hor-from-xs u-padding-s-hor-from-md u-padding-xs-ver text-xs u-clear-both-from-xs u-clear-none-from-md"><p class="u-margin-xs-bottom"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/solutions/sciencedirect" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">About ScienceDirect</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.sciencedirect.com/customer/authenticate/manra" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Remote access</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.sciencedirect.com/science?_ob=ShoppingCartURL&amp;_method=display&amp;md5=3ff44acb300f01481824c54a2973d019" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Shopping cart</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Contact and support</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Terms and conditions</span></a><wbr><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/privacy-policy" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">Privacy policy</span></a></p><p id="els-footer-cookie-message">We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the<!-- --> <a class="anchor u-margin-0-right" href="https://www.sciencedirect.com/legal/use-of-cookies" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text">use of cookies</span></a>.</p><p id="els-footer-copyright">Copyright Â© <!-- -->2018<!-- --> Elsevier B.V. or its licensors or contributors. ScienceDirect Â® is a registered trademark of Elsevier B.V.</p></div><div><span style="margin-top:-10px" class="u-position-relative move-bottom u-float-left-from-xs u-float-right-from-md move-right u-padding-0-hor"><a class="anchor els-footer-relx anchor-has-inherit-color" href="https://www.relx.com/" rel="nofollow" target="_blank" style="white-space:nowrap"><span class="anchor-text"><svg style="width:118px;height:28px" viewBox="-3248 3424.938 334.44 53.422"><title>RELX Group</title><path style="fill:#E9711C" d="M-3227.285,3448.329c10.948,0,24.15-2.576,24.15-13.124c0-7.474-7.63-10.267-14.6-10.267 c-10.647,0-30.265,5.926-30.265,26.96c0,10.555,8.034,16.643,20.778,16.643c13.474,0,23.266-8.768,26.26-20.535 c-7.912,13.648-17.972,17.896-26.18,17.896c-10.19,0-14.077-7.568-14.077-14.05c0-18.56,14.406-24.798,23.494-24.798 c6.135,0,9.727,3.936,9.727,8.253c0,11.918-17.17,11.578-25.008,11.578c-0.803,0-3.274,0.02-3.76-0.003l-1.654,1.714 c3.59,0.508,8.554,1.497,13.944,4.223c11.204,5.656,21.745,20.59,35.7,20.59c5.062,0,6.38-1.543,7.335-2.893 c-6.834,4.354-18.286-3.197-24.46-9.04c-5.082-4.806-9.57-10.26-21.387-13.146"></path><g><path style="fill:#666666" d="M-3158.73,3468.502c-0.423,0-0.636-0.164-0.794-0.547l-6.885-14.511h-6.197c-0.21,0-0.316,0.11-0.316,0.33 v14.182c0,0.328-0.213,0.547-0.532,0.547h-5.189c-0.32,0-0.53-0.22-0.53-0.547v-36.139c0-0.33,0.21-0.55,0.53-0.549h13.928 c6.62,0,11.492,4.545,11.492,11.117c0,4.873-2.7,8.65-6.833,10.238l7.575,15.167c0.21,0.385,0,0.713-0.37,0.713L-3158.73,3468.502 L-3158.73,3468.502z M-3159.42,3442.384c0-3.232-2.223-5.31-5.506-5.311h-7.68c-0.21,0-0.316,0.108-0.316,0.328v9.912 c0,0.217,0.104,0.328,0.316,0.328h7.68C-3161.644,3447.641-3159.42,3445.561-3159.42,3442.384"></path><path style="fill:#666666" d="M-3146.95,3431.817c0-0.33,0.21-0.55,0.53-0.549h21.865c0.317,0,0.53,0.22,0.53,0.549v4.709 c0,0.33-0.212,0.548-0.53,0.548h-15.827c-0.212,0-0.318,0.108-0.318,0.328v9.089c0,0.22,0.106,0.328,0.318,0.328h12.726 c0.317,0,0.53,0.22,0.53,0.547v4.709c0,0.33-0.213,0.548-0.53,0.548h-12.726c-0.212,0-0.318,0.11-0.318,0.329v9.418 c0,0.22,0.106,0.328,0.318,0.328h15.827c0.317,0,0.53,0.22,0.53,0.549v4.709c0,0.328-0.212,0.547-0.53,0.547h-21.865 c-0.32,0-0.53-0.22-0.53-0.547C-3146.95,3467.956-3146.95,3431.817-3146.95,3431.817z"></path><path style="fill:#666666" d="M-3118.2,3431.817c0-0.33,0.212-0.55,0.53-0.549h5.191c0.317,0,0.53,0.22,0.53,0.549v30.553 c0,0.22,0.106,0.328,0.317,0.328h14.722c0.318,0,0.53,0.22,0.53,0.549v4.709c0,0.328-0.213,0.547-0.53,0.547h-20.76 c-0.317,0-0.53-0.22-0.53-0.547C-3118.2,3467.956-3118.2,3431.817-3118.2,3431.817z"></path><path style="fill:#666666" d="M-3052.46,3449.885c0-6.46,0.423-8.926,1.006-10.787c1.747-5.53,5.878-8.377,11.758-8.377 c5.88,0,9.532,3.012,11.28,6.9c0.107,0.274,0.107,0.548-0.21,0.712l-1.695,0.876c-0.265,0.11-0.53,0.055-0.688-0.22 c-1.802-3.396-4.45-5.312-8.74-5.312c-4.45,0-7.468,2.192-8.792,6.3c-0.475,1.422-0.845,3.722-0.845,9.909s0.37,8.486,0.845,9.91 c1.324,4.107,4.343,6.3,8.792,6.3c4.342,0,7.468-2.137,8.79-6.19c0.477-1.422,0.85-3.56,0.85-7.117c0-0.22-0.106-0.328-0.318-0.328 h-8.155c-0.318,0-0.53-0.22-0.53-0.548v-1.807c0-0.33,0.212-0.548,0.53-0.548h10.962c0.317,0,0.53,0.22,0.53,0.548v2.243 c0,3.725-0.423,6.572-0.954,8.27c-1.694,5.53-5.88,8.432-11.65,8.432c-5.88,0-10.01-2.848-11.758-8.38 C-3052.036,3458.811-3052.459,3456.345-3052.46,3449.885"></path><path style="fill:#666666" d="M-3021.43,3468.502c-0.317,0-0.53-0.22-0.53-0.547v-25.023c0-0.33,0.213-0.548,0.53-0.548h1.801 c0.316,0,0.53,0.22,0.53,0.548v3.338h0.052c0.953-2.572,3.23-4.434,6.78-4.434c2.013,0,3.866,0.712,5.084,1.807 c0.265,0.164,0.317,0.438,0.107,0.713l-1.06,1.531c-0.212,0.274-0.48,0.274-0.795,0.11c-1.164-0.767-2.436-1.313-4.024-1.313 c-4.45,0-6.144,3.996-6.144,8.815v14.456c0,0.328-0.213,0.547-0.53,0.547H-3021.43L-3021.43,3468.502z"></path><path style="fill:#666666" d="M-3004.54,3462.26c-0.53-1.752-0.847-3.668-0.847-6.844c0-3.12,0.316-5.037,0.847-6.79 c1.378-4.327,4.818-6.79,9.426-6.79c4.662,0,8.104,2.464,9.48,6.79c0.53,1.752,0.848,3.668,0.848,6.79 c0,3.176-0.317,5.092-0.848,6.844c-1.377,4.326-4.818,6.79-9.48,6.79C-2999.722,3469.05-3003.162,3466.585-3004.54,3462.26 M-2988.387,3461.33c0.475-1.48,0.688-3.066,0.688-5.914c0-2.794-0.213-4.38-0.688-5.86c-1.007-3.12-3.442-4.873-6.728-4.873 c-3.23,0-5.666,1.752-6.672,4.873c-0.477,1.48-0.688,3.065-0.688,5.86c0,2.848,0.212,4.436,0.688,5.914 c1.006,3.12,3.442,4.873,6.672,4.873C-2991.829,3466.203-2989.395,3464.451-2988.387,3461.33"></path><path style="fill:#666666" d="M-2963.66,3468.502c-0.317,0-0.528-0.22-0.528-0.547v-3.066h-0.053c-1.272,2.574-3.92,4.162-7.36,4.162 c-5.615,0-8.74-3.616-8.74-9.913v-16.206c0-0.33,0.213-0.548,0.53-0.548h1.801c0.317,0,0.53,0.22,0.53,0.548v15.44 c0,5.257,2.118,7.83,6.515,7.83c3.812,0,6.78-2.74,6.78-7.284v-15.987c0-0.33,0.21-0.548,0.528-0.548h1.802 c0.317,0,0.53,0.22,0.53,0.548v25.023c0,0.328-0.213,0.547-0.53,0.547h-1.802L-2963.66,3468.502z"></path><path style="fill:#666666" d="M-2955.29,3478.36c-0.317,0-0.53-0.22-0.53-0.549v-34.879c0-0.33,0.213-0.548,0.53-0.548h1.8 c0.317,0,0.53,0.22,0.53,0.548v3.176h0.053c1.218-2.465,3.496-4.272,7.68-4.272c4.45,0,7.256,2.08,8.58,6.242 c0.638,2.137,0.9,4.435,0.9,7.393c0,2.9-0.264,5.2-0.9,7.336c-1.323,4.164-4.13,6.242-8.58,6.242c-4.183,0-6.46-1.807-7.68-4.27 h-0.053v13.031c0,0.33-0.213,0.55-0.53,0.549L-2955.29,3478.36L-2955.29,3478.36z M-2939.35,3461.602 c0.53-1.643,0.688-3.832,0.688-6.13c0-2.355-0.158-4.545-0.688-6.188c-0.952-2.957-3.124-4.6-6.46-4.6 c-3.178,0-5.507,1.533-6.46,4.6c-0.477,1.423-0.69,3.34-0.69,6.188c0,2.847,0.212,4.71,0.69,6.13 c0.954,3.067,3.283,4.602,6.46,4.602C-2942.473,3466.204-2940.3,3464.561-2939.35,3461.602"></path><path style="fill:#666666" d="M-3070.48,3468.545c-0.424,0-0.634-0.164-0.85-0.547l-7.254-12.649h-0.106l-7.307,12.649 c-0.213,0.383-0.424,0.547-0.85,0.547h-5.72c-0.37,0-0.528-0.328-0.318-0.711l10.76-18.562l-9.96-17.248 c-0.21-0.385-0.05-0.712,0.32-0.712h5.721c0.424,0,0.636,0.163,0.847,0.548l6.513,11.278h0.106l6.513-11.278 c0.213-0.385,0.425-0.548,0.85-0.548h5.72c0.37,0,0.53,0.327,0.317,0.712l-9.956,17.248l10.75,18.562 c0.21,0.383,0.052,0.71-0.32,0.711h-5.771H-3070.48z"></path><path style="fill:#666666" d="M-2932.08,3443.951c-0.108,0-0.18-0.075-0.18-0.189v-10.385c0-0.075-0.036-0.113-0.11-0.113h-3.202 c-0.11,0-0.184-0.073-0.184-0.186v-1.6c0-0.11,0.074-0.186,0.184-0.186h8.746c0.11,0,0.183,0.075,0.183,0.186v1.6 c0,0.113-0.07,0.186-0.183,0.186h-3.201c-0.073,0-0.11,0.038-0.11,0.113v10.385c0,0.113-0.07,0.19-0.18,0.189H-2932.08 L-2932.08,3443.951z"></path><path style="fill:#666666" d="M-2924.47,3431.478c0-0.112,0.07-0.186,0.178-0.186h1.604c0.163,0,0.252,0.056,0.307,0.186l3.367,7.8h0.07 l3.315-7.8c0.052-0.13,0.144-0.186,0.305-0.186h1.583c0.11,0,0.182,0.074,0.182,0.186v12.286c0,0.11-0.07,0.186-0.182,0.186h-1.564 c-0.11,0-0.18-0.075-0.18-0.186v-7.744h-0.073l-2.593,5.957c-0.07,0.168-0.18,0.242-0.343,0.242h-1.044 c-0.16,0-0.27-0.074-0.343-0.242l-2.59-5.957h-0.072v7.744c0,0.11-0.072,0.186-0.18,0.186h-1.567c-0.106,0-0.178-0.075-0.178-0.186 v-12.286H-2924.47z"></path></g></svg></span></a></span></div></div></div></section></div></div></div>
<script type="application/json" data-iso-key="_0">{"userAgent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0","abstracts":{"content":[{"#name":"abstract","$":{"xmlns:ce":true,"lang":"en","id":"abs0010","view":"all","class":"author"},"$$":[{"#name":"section-title","$":{"id":"sectitle0010a"},"_":"Abstract"},{"#name":"abstract-sec","$":{"id":"abssec0010","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"abspara0010","view":"all"},"_":"The promising technology of stereoscopic displays is interesting to explore because 3D virtual applications are widely known. Thus, this study investigated the effect of parallax on eye fixation in stereoscopic displays. The experiment was conducted in three different levels of parallax, in which virtual balls were projected at the screen, at 20â€¯cm and 50â€¯cm in front the screen. The two important findings of this study are that parallax has significant effects on fixation duration, time to first fixation, number of fixations, and accuracy. The participant had more accurate fixations, fewer fixations, shorter fixation durations, and shorter times to first fixation when the virtual ball was projected at the screen than when it was projected at the other two levels of parallax."}]}]},{"#name":"abstract","$":{"xmlns:ce":true,"class":"graphical","id":"abs0015","view":"all"},"$$":[{"#name":"section-title","$":{"id":"sectitle0010"},"_":"Graphical abstract"},{"#name":"abstract-sec","$":{"id":"abssec0015","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"abspara0015","view":"all"},"$$":[{"#name":"__text__","_":"While a person is looking at targets displayed in a virtual environment (a stereoscopic display), the eye movement is recorded and analysed, and the interaction performance using a clicking task is examined."},{"#name":"display","$$":[{"#name":"figure","$":{"id":"undfig1"},"$$":[{"#name":"alt-text","$":{"role":"short","id":"alttext0010"},"_":"Image 1"},{"#name":"link","$":{"xmlns:xlink":true,"locator":"fx1","type":"simple","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","href":"pii:S0003687017302867/fx1","id":"aep-link-id8"}}]}]}]}]}]},{"#name":"abstract","$":{"xmlns:ce":true,"class":"author-highlights","lang":"en","id":"abs0020","view":"all"},"$$":[{"#name":"section-title","$":{"id":"sectitle0015"},"_":"Highlights"},{"#name":"abstract-sec","$":{"id":"abssec0020","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"abspara0020","view":"all"},"$$":[{"#name":"list","$":{"id":"ulist0010"},"$$":[{"#name":"list-item","$":{"id":"u0010"},"$$":[{"#name":"label","_":"â€¢"},{"#name":"para","$":{"id":"p0010","view":"all"},"_":"Eye movement was analyzed in three different levels of object depth (parallax) in a projection-based stereoscopic display."}]},{"#name":"list-item","$":{"id":"u0015"},"$$":[{"#name":"label","_":"â€¢"},{"#name":"para","$":{"id":"p0015","view":"all"},"_":"Significant effects of parallax were found."}]},{"#name":"list-item","$":{"id":"u0020"},"$$":[{"#name":"label","_":"â€¢"},{"#name":"para","$":{"id":"p0020","view":"all"},"_":"Virtual objects at the screen results in better accuracy."}]},{"#name":"list-item","$":{"id":"u0025"},"$$":[{"#name":"label","_":"â€¢"},{"#name":"para","$":{"id":"p0025","view":"all"},"_":"The result contributes to designing effective interactive tasks where different targets appear in the stereoscopic display."}]}]}]}]}]}],"floats":[],"footnotes":[],"attachments":[{"attachment-eid":"1-s2.0-S0003687017302867-fx1.sml","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0003687017302867/fx1/THUMBNAIL/image/gif/3a12068b9685756f1463b8a56015d38c/fx1.sml","file-basename":"fx1","abstract-attachment":"true","filename":"fx1.sml","extension":"sml","filesize":"11322","pixel-height":"109","pixel-width":"219","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0003687017302867-fx1.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0003687017302867/fx1/DOWNSAMPLED/image/jpeg/f0e63ad18db82f95d3f286901a45ca82/fx1.jpg","file-basename":"fx1","abstract-attachment":"true","filename":"fx1.jpg","extension":"jpg","filesize":"30308","pixel-height":"200","pixel-width":"402","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0003687017302867-fx1_lrg.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0003687017302867/fx1/HIGHRES/image/jpeg/45c63f36beca33cab9becb0cb61e70d8/fx1_lrg.jpg","file-basename":"fx1","abstract-attachment":"true","filename":"fx1_lrg.jpg","extension":"jpg","filesize":"136980","pixel-height":"886","pixel-width":"1779","attachment-type":"IMAGE-HIGH-RES"}]},"biographies":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"lang":"en","id":"kwrds0010","view":"all","class":"keyword"},"$$":[{"#name":"section-title","$":{"id":"sectitle0020"},"_":"Keywords"},{"#name":"keyword","$":{"id":"kwrd0010"},"$$":[{"#name":"text","_":"Depth perception"}]},{"#name":"keyword","$":{"id":"kwrd0015"},"$$":[{"#name":"text","_":"Eye fixation"}]},{"#name":"keyword","$":{"id":"kwrd0020"},"$$":[{"#name":"text","_":"3D tasks"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"experiments":{},"rawtext":"","authors":{"content":[{"#name":"author-group","$":{"xmlns:ce":true,"id":"augrp0010"},"$$":[{"#name":"author","$":{"id":"au1","author-id":"S0003687017302867-0177f0c851c345a32d6625550ba37999"},"$$":[{"#name":"given-name","_":"Chiuhsiang Joe"},{"#name":"surname","_":"Lin"},{"#name":"cross-ref","$":{"refid":"cor1","id":"crosref0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âˆ—"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:cjoelin@mail.ntust.edu.tw","id":"eadd0010"},"_":"cjoelin@mail.ntust.edu.tw"}]},{"#name":"author","$":{"id":"au2","author-id":"S0003687017302867-cb6209e20a1b79ddf86b0ef58ee4dacc"},"$$":[{"#name":"given-name","_":"Retno"},{"#name":"surname","_":"Widyaningrum"}]},{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S0003687017302867-359e64d6571dd4894c511148956d15f2"},"$$":[{"#name":"textfn","_":"Department of Industrial Management, National Taiwan University of Science and Technology, No.43, Sec. 4, Keelung Rd., Da'an Dist., Taipei 10607, Taiwan, ROC"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Industrial Management"},{"#name":"organization","_":"National Taiwan University of Science and Technology"},{"#name":"address-line","_":"No.43, Sec. 4"},{"#name":"address-line","_":"Keelung Rd."},{"#name":"address-line","_":"Da'an Dist."},{"#name":"city","_":"Taipei"},{"#name":"postal-code","_":"10607"},{"#name":"country","_":"Taiwan, ROC"}]}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âˆ—"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff1":{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S0003687017302867-359e64d6571dd4894c511148956d15f2"},"$$":[{"#name":"textfn","_":"Department of Industrial Management, National Taiwan University of Science and Technology, No.43, Sec. 4, Keelung Rd., Da'an Dist., Taipei 10607, Taiwan, ROC"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Industrial Management"},{"#name":"organization","_":"National Taiwan University of Science and Technology"},{"#name":"address-line","_":"No.43, Sec. 4"},{"#name":"address-line","_":"Keelung Rd."},{"#name":"address-line","_":"Da'an Dist."},{"#name":"city","_":"Taipei"},{"#name":"postal-code","_":"10607"},{"#name":"country","_":"Taiwan, ROC"}]}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âˆ—"},{"#name":"text","_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"body":{},"exam":{},"article":{"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":52,"openArchiveStatus":false,"openArchiveArticleCount":0,"openAccessStartDate":"","oaAllowsAuthorPaid":false},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0003687018X00027-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0003687018X00027/cover/DOWNSAMPLED200/image/gif/f419f03b2d4490448bb90eebf0e12df3/cov200h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0003687018X00027/cover/DOWNSAMPLED200/image/gif/f419f03b2d4490448bb90eebf0e12df3/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"27301","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S0003687018X00027-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0003687018X00027/cover/DOWNSAMPLED/image/gif/b80a3e136222b06621c5392fd2579761/cov150h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0003687018X00027/cover/DOWNSAMPLED/image/gif/b80a3e136222b06621c5392fd2579761/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"20285","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S00036870.gif","sourceOpenAccess":false,"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0003687018X00027-cov150h.gif"},"pii":"S0003687017302867","dates":{"Available online":"6 January 2018","Received":"15 August 2017","Revised":["20 November 2017"],"Accepted":"29 December 2017","Publication date":"1 May 2018"},"access":{"openArchive":false,"openAccess":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"document-references":52,"analyticsMetadata":{"accountId":"47346","accountName":"Galway Mayo Institute of Technology","loginStatus":"anonymous","userId":"12543123"},"cid":"271441","content-family":"serial","copyright-line":"Â© 2018 Elsevier Ltd. All rights reserved.","cover-date-years":["2018"],"cover-date-start":"2018-05-01","cover-date-text":"May 2018","document-subtype":"fla","document-type":"article","entitledToken":"9A5C6108E5817C16C747029C5BD8737C7F5D7C163B57E153339B51A29DCA0AA81D2256F16982EBCA","eid":"1-s2.0-S0003687017302867","doi":"10.1016/j.apergo.2017.12.020","first-fp":"10","hub-eid":"1-s2.0-S0003687018X00027","issuePii":"S0003687018X00027","item-weight":"FULL-TEXT","language":"en","last-lp":"16","last-author":{"#name":"last-author","$":{"xmlns:dm":true},"$$":[{"#name":"author","$":{"xmlns:ce":true,"id":"au2","author-id":"S0003687017302867-cb6209e20a1b79ddf86b0ef58ee4dacc"},"$$":[{"#name":"given-name","_":"Retno"},{"#name":"surname","_":"Widyaningrum"}]}]},"normalized-first-auth-initial":"C","normalized-first-auth-surname":"LIN","pages":[{"last-page":"16","first-page":"10"}],"srctitle":"Applied Ergonomics","suppl":"C","timestamp":"2018-02-21T13:08:46.096666Z","title":{"content":[{"#name":"title","$":{"xmlns:ce":true,"id":"title0010"},"_":"The effect of parallax on eye fixation parameter in projection-based stereoscopic displays"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"69","vol-iss-suppl-text":"Volume 69","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"issn":"00036870","issn-primary-formatted":"0003-6870","useEnhancedReader":true,"isCorpReq":false,"pdfDownload":{"linkType":"DOWNLOAD","linkToPdf":"/science/article/pii/S0003687017302867/pdfft?md5=af42c555a80c115353af2b16ffca109c&pid=1-s2.0-S0003687017302867-main.pdf","isPdfFullText":false,"fileName":"1-s2.0-S0003687017302867-main.pdf"},"pdfUrlForCrawlers":"https://www.sciencedirect.com/science/article/pii/S0003687017302867/pdfft?md5=af42c555a80c115353af2b16ffca109c&pid=1-s2.0-S0003687017302867-main.pdf","indexTag":true,"volRange":"69","issRange":"","ssoUrls":["//acw.scopus.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&utt=22f3-2563b5f9761d42d11e2-691bd6251328b749","//acw.sciencedirect.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&utt=22f3-2563b5f9761d42d11e2-691bd6251328b749","//acw.evise.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&utt=22f3-2563b5f9761d42d11e2-691bd6251328b749","//acw.elsevier.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&utt=22f3-2563b5f9761d42d11e2-691bd6251328b749"],"userProfile":{"departmentName":"Shibboleth (401002)","accessType":"SHIBANON","accountId":"47346","webUserId":"12543123","libraryBanner":{"text":"GMIT Library","position":"RIGHT","url":"http://gmitlib.gmit.ie"},"accountName":"Galway Mayo Institute of Technology","shibProfile":{"canRegister":true,"institutionId":"95083","institutionName":"Galway-Mayo Institute of Technology","federationId":"9028"},"departmentId":"272408","userType":"NORMAL","hasMultipleOrganizations":false,"registrationUrl":"/customer/profile/display?targetURL=%2Fscience%2Farticle%2Fpii%2FS0003687017302867%3F"},"entitlementReason":"package","articleEntitlement":{"entitled":true,"usageInfo":"(12543123,U|272408,D|47346,A|34,P|2,PL)(SDFE,CON|6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb,SSO|ANON_SHIBBOLETH,ACCESS_TYPE)"},"aipType":"none","downloadFullIssue":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/app/home/supporthub/sciencedirect/","contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"My account","orgName":"Galway Mayo Institute of Technology, Shibboleth (401002)","webUserId":"12543123","libraryBanner":{"libraryBannerText":"GMIT Library","libraryBannerUrl":"http://gmitlib.gmit.ie","position":"RIGHT"},"shib_regUrl":"/customer/profile/display?targetURL=%2Fscience%2Farticle%2Fpii%2FS0003687017302867%3F","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":true,"hasMultiOrg":false,"userType":"SHIBANON","allowCart":true,"institutionName":"Galway-Mayo Institute of Technology","institutionId":"95083","federationId":"9028"},"open-research":{},"titleString":"The effect of parallax on eye fixation parameter in projection-based stereoscopic displays","onAbstractWhitelist":false,"isAbstract":false,"isContentVisible":false,"ajaxLinks":{"citingArticles":true,"references":true,"referredToBy":true,"toc":true,"body":true,"recommendations":true}},"specialIssueArticles":{},"recommendations":{},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"citingArticles":{},"workspace":{"isOpen":false},"crossMark":{"isOpen":false},"userIdentity":{},"refersTo":{},"referredToBy":{},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[]},"references":{},"referenceLinks":{"internal":{},"external":{}},"glossary":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenRecommendations":true,"isOpenCitingArticles":false,"citingArticles":[false,false,false],"recommendations":[false,false,false,false,false,false],"specialIssueArticles":[false,false,false]},"banner":{"expanded":false},"transientError":{"isOpen":false},"tableOfContents":{"showEntitledTocLinks":true},"chapters":{"toc":[],"isLoading":false},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"metrics":{"isOpen":true},"signOut":{"isOpen":false},"issueNavigation":{"previous":{},"next":{}},"tail":{},"linkingHubLinks":[],"signInFromEmail":{"isOpen":false}}</script>
      <iframe style="display: none" src="//acw.scopus.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&amp;utt=22f3-2563b5f9761d42d11e2-691bd6251328b749" tabindex="-1"></iframe><iframe style="display: none" src="//acw.sciencedirect.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&amp;utt=22f3-2563b5f9761d42d11e2-691bd6251328b749" tabindex="-1"></iframe><iframe style="display: none" src="//acw.evise.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&amp;utt=22f3-2563b5f9761d42d11e2-691bd6251328b749" tabindex="-1"></iframe><iframe style="display: none" src="//acw.elsevier.com/SSOCore/update?acw=6f350a4e6d7cd647772a49a71b842e6d1b66gxrqb%7C%24%7C86A4763F57188DF8D77CAA56CFC72D78B2AC919B929652382F3433D23C8CF6961E896ED81A98053592F9C2460B97E98FBB34E150ACAFA0F70E9169905BBD791C8EA87459D0257BAAEEC97078780D6ECD26CF2F237D0C127D10B1D490FCC5DDB2&amp;utt=22f3-2563b5f9761d42d11e2-691bd6251328b749" tabindex="-1"></iframe>
      <script src="satelliteLib-b7cfe8df39a4e5eec5536bba80e13f4b6fa0dd7c.js"></script><script src="satellite-565e008964746d4385002642.js"></script>
      <script src="babel-polyfill.js"></script>
      <script src="react.js"></script>
      <script src="react-dom.js"></script>
      <script src="arp.js" async=""></script>
      <!-- begin usabilla live embed code -->
      <script type="text/javascript">
        window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
        window.usabilla_live = lightningjs.require("usabilla_live", "https://w.usabilla.com/eb1c14a91932.js");
        var customData = {};

        if(window.pageData && pageData.content && pageData.content[0]) {
          customData.entitlementType = pageData.content[0].entitlementType;
        }
        if(window.pageData && pageData.visitor) {
          customData.accessType = pageData.visitor.accessType;
          customData.accountId = pageData.visitor.accountId;
          customData.loginStatus = pageData.visitor.loginStatus;
        }
        usabilla_live("data", {"custom": customData });
      </script>
      <!-- end usabilla live embed code -->
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="MathJax.js"></script>
      <script async="" src="gpt.js"></script>
    
  <div class="js-react-modal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><div class="ReactModalPortal"></div><script src="widget-summary.js" async=""></script></body></html>